import pandas as pd
from tableauhyperapi import TableName, HyperProcess, Connection, Telemetry
import os
import numpy as np
from datetime import datetime
import pantab as pt # å¯¼å…¥ pantab åº“

def convert_hyper_to_csv(hyper_file_path, csv_output_path):
    """
    å°†.hyperæ–‡ä»¶è½¬æ¢ä¸ºCSVæ–‡ä»¶
    
    Args:
        hyper_file_path: è¾“å…¥çš„.hyperæ–‡ä»¶è·¯å¾„
        csv_output_path: è¾“å‡ºçš„CSVæ–‡ä»¶è·¯å¾„
    """
    print(f"ğŸ”„ æ­£åœ¨å¤„ç†æ–‡ä»¶: {hyper_file_path}")
    
    # æ­¥éª¤1ï¼šä».hyperæ–‡ä»¶è¯»å–æ•°æ®
    try:
        # é¦–å…ˆä½¿ç”¨ tableauhyperapi æŸ¥çœ‹æ–‡ä»¶ä¸­çš„å®é™…è¡¨å
        print("ğŸ” æ­£åœ¨æ£€æŸ¥ Hyper æ–‡ä»¶ä¸­çš„è¡¨ç»“æ„...")
        table_name = None
        
        with HyperProcess(telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU) as hyper:
            with Connection(endpoint=hyper.endpoint, database=hyper_file_path) as connection:
                # è·å–æ‰€æœ‰ schema
                schemas = connection.catalog.get_schema_names()
                print(f"ğŸ“‹ å‘ç°çš„ schema: {[str(schema) for schema in schemas]}")
                
                for schema in schemas:
                    try:
                        # è·å–è¯¥ schema ä¸‹çš„æ‰€æœ‰è¡¨å
                        tables = connection.catalog.get_table_names(schema)
                        print(f"ğŸ“‹ Schema '{schema}' ä¸­çš„è¡¨: {[str(table) for table in tables]}")
                        
                        if tables:
                            # ä½¿ç”¨ç¬¬ä¸€ä¸ªè¡¨ï¼Œæ¸…ç†å¼•å·
                            raw_table_name = str(tables[0])
                            # ç§»é™¤å¤šä½™çš„å¼•å·
                            table_name = raw_table_name.replace('"', '')
                            print(f"ğŸ¯ æ‰¾åˆ°è¡¨å: {raw_table_name} -> æ¸…ç†å: {table_name}")
                            break
                    except Exception as schema_error:
                        print(f"âš ï¸ æ£€æŸ¥ schema '{schema}' æ—¶å‡ºé”™: {schema_error}")
                        continue
        
        # å°è¯•è®© pantab è‡ªåŠ¨æ£€æµ‹è¡¨
        try:
            print("ğŸ“– å°è¯•è®© pantab è‡ªåŠ¨æ£€æµ‹è¡¨...")
            df = pt.frame_from_hyper(hyper_file_path)
            print(f"ğŸ“Š æˆåŠŸä» Hyper æ–‡ä»¶è¯»å– {len(df)} è¡Œæ•°æ®åˆ° DataFrameï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰ã€‚")
        except Exception as auto_error:
            print(f"âŒ è‡ªåŠ¨æ£€æµ‹å¤±è´¥: {auto_error}")
            
            # å¦‚æœè‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹çš„ tableauhyperapi æ–¹æ³•
            print("ğŸ”„ ä½¿ç”¨ tableauhyperapi ç›´æ¥è¯»å–æ•°æ®...")
            with HyperProcess(telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU) as hyper:
                with Connection(endpoint=hyper.endpoint, database=hyper_file_path) as connection:
                    # ä½¿ç”¨åŸå§‹è¡¨åæ„é€ æŸ¥è¯¢
                    raw_table_name = '"Extract"."Extract"'
                    query = f"SELECT * FROM {raw_table_name}"
                    print(f"ğŸ“– æ‰§è¡ŒæŸ¥è¯¢: {query}")
                    
                    result = connection.execute_query(query)
                    
                    # è·å–åˆ—å
                    column_names = [col.name.unescaped for col in result.schema.columns]
                    print(f"ğŸ“‹ åˆ—å: {column_names}")
                    
                    # åˆ†å—è¯»å–å¹¶ç›´æ¥å†™å…¥CSVæ–‡ä»¶ä»¥é¿å…å†…å­˜é—®é¢˜
                    chunk_size = 50000  # å¢å¤§å—å¤§å°æé«˜æ•ˆç‡
                    rows = []
                    row_count = 0
                    chunk_count = 0
                    temp_files = []
                    
                    print(f"ğŸ“Š å¼€å§‹åˆ†å—è¯»å–å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆæ¯å— {chunk_size} è¡Œï¼‰...")
                    
                    try:
                        for row in result:
                            # å¤„ç†æ—¥æœŸç±»å‹è½¬æ¢
                            processed_row = []
                            for i, value in enumerate(row):
                                if hasattr(value, '__class__') and 'Date' in str(type(value)):
                                    # å°†Tableauçš„Dateå¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                                    processed_row.append(str(value))
                                else:
                                    processed_row.append(value)
                            
                            rows.append(processed_row)
                            row_count += 1
                            
                            # æ¯è¯»å–ä¸€å®šæ•°é‡çš„è¡Œå°±å†™å…¥ä¸´æ—¶æ–‡ä»¶
                            if len(rows) >= chunk_size:
                                chunk_df = pd.DataFrame(rows, columns=column_names)
                                
                                temp_file = f"{csv_output_path}.temp_{chunk_count}.csv"
                                chunk_df.to_csv(temp_file, index=False)
                                temp_files.append(temp_file)
                                
                                print(f"ğŸ“ˆ å·²å¤„ç† {row_count} è¡Œï¼Œå†™å…¥ä¸´æ—¶æ–‡ä»¶ {chunk_count + 1}...")
                                
                                rows = []
                                chunk_count += 1
                        
                        # å¤„ç†æœ€åä¸€å—æ•°æ®
                        if rows:
                            chunk_df = pd.DataFrame(rows, columns=column_names)
                            
                            temp_file = f"{csv_output_path}.temp_{chunk_count}.csv"
                            chunk_df.to_csv(temp_file, index=False)
                            temp_files.append(temp_file)
                            chunk_count += 1
                    finally:
                        # ç¡®ä¿å…³é—­result
                        if 'result' in locals():
                            try:
                                result.close()
                            except:
                                pass
                    
                    print(f"ğŸ“Š æ€»å…±è¯»å– {row_count} è¡Œæ•°æ®ï¼Œåˆ†ä¸º {chunk_count} ä¸ªä¸´æ—¶æ–‡ä»¶")
                    
                    # åˆå¹¶æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
                    print("ğŸ”„ æ­£åœ¨åˆå¹¶ä¸´æ—¶æ–‡ä»¶...")
                    all_chunks = []
                    for temp_file in temp_files:
                        chunk_df = pd.read_csv(temp_file)
                        all_chunks.append(chunk_df)
                    
                    df = pd.concat(all_chunks, ignore_index=True)
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    for temp_file in temp_files:
                        try:
                            os.remove(temp_file)
                        except:
                            pass
                    
                    print(f"âœ… æˆåŠŸåˆå¹¶æ‰€æœ‰æ•°æ®ï¼Œæ€»å…± {len(df)} è¡Œã€‚")

    except Exception as e:
        print(f"âŒ è¯»å–Hyperæ–‡ä»¶å¤±è´¥: {e}")
        return False
                
    
    # æ­¥éª¤2ï¼šä¿å­˜ä¸ºCSVæ–‡ä»¶
    try:
        output_dir = os.path.dirname(csv_output_path)
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜CSVæ–‡ä»¶: {csv_output_path}")
        df.to_csv(csv_output_path, index=False, encoding='utf-8-sig')
        
        # éªŒè¯ä¿å­˜ç»“æœ
        file_size = os.path.getsize(csv_output_path) / (1024 * 1024)  # MB
        print(f"âœ… è½¬æ¢å®Œæˆï¼æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        # æ•°æ®è´¨é‡æŠ¥å‘Š
        print_data_quality_report(df)
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
        return False

# æ•°æ®ç±»å‹ä¼˜åŒ–åŠŸèƒ½å·²ç§»è‡³ csv_to_parquet.py è„šæœ¬ä¸­å¤„ç†

def print_data_quality_report(df):
    """
    æ‰“å°æ•°æ®è´¨é‡æŠ¥å‘Š
    """
    print("\nğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Š:")
    print(f"  ğŸ“ æ•°æ®ç»´åº¦: {df.shape[0]} è¡Œ x {df.shape[1]} åˆ—")
    
    # ç¼ºå¤±å€¼ç»Ÿè®¡
    missing_stats = df.isnull().sum()
    if missing_stats.sum() > 0:
        print("\nâš ï¸  ç¼ºå¤±å€¼ç»Ÿè®¡:")
        for col, missing_count in missing_stats[missing_stats > 0].items():
            missing_pct = (missing_count / len(df)) * 100
            print(f"    {col}: {missing_count} ({missing_pct:.1f}%)")
    else:
        print("\nâœ… æ— ç¼ºå¤±å€¼")
    
    # æ•°æ®ç±»å‹ç»Ÿè®¡
    type_counts = df.dtypes.value_counts()
    print("\nğŸ“‹ æ•°æ®ç±»å‹åˆ†å¸ƒ:")
    for dtype, count in type_counts.items():
        print(f"    {dtype}: {count} åˆ—")
    
    # å†…å­˜ä½¿ç”¨æƒ…å†µ
    memory_usage = df.memory_usage(deep=True).sum() / (1024 * 1024)
    print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨: {memory_usage:.2f} MB")

if __name__ == "__main__":
    # æ–‡ä»¶è·¯å¾„é…ç½®
    hyper_file = "/Users/zihao_/Documents/coding/dataset/original/ä¹˜ç”¨è½¦ä¸Šé™©é‡_0723.hyper"
    csv_file = "/Users/zihao_/Documents/coding/dataset/formatted/ä¹˜ç”¨è½¦ä¸Šé™©é‡_0723.csv"
    
    # æ£€æŸ¥ä¾èµ–åº“
    try:
        from tableauhyperapi import TableName
        print("âœ… tableauhyperapi å·²å®‰è£…")
    except ImportError:
        print("âŒ é”™è¯¯: 'tableauhyperapi' åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install tableauhyperapi")
        exit(1)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(hyper_file):
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {hyper_file}")
        exit(1)
    
    print("ğŸš€ å¼€å§‹Hyperåˆ°CSVè½¬æ¢...")
    print("=" * 50)
    
    # æ‰§è¡Œè½¬æ¢
    success = convert_hyper_to_csv(
        hyper_file_path=hyper_file,
        csv_output_path=csv_file
    )
    
    if success:
        print("\nğŸ‰ è½¬æ¢æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {csv_file}")
        print("\nğŸ’¡ æç¤º: å¦‚éœ€è½¬æ¢ä¸ºParquetæ ¼å¼ï¼Œè¯·è¿è¡Œ csv_to_parquet.py")
    else:
        print("\nâŒ è½¬æ¢å¤±è´¥")
        exit(1)
