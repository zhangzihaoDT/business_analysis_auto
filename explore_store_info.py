#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åº—é“ºä¿¡æ¯æ•°æ®æ¢ç´¢åˆ†æ

è¯¥è„šæœ¬ç”¨äºåŠ è½½å’Œåˆ†æ store_info_data.csv æ•°æ®ï¼Œè¿›è¡ŒåŸºæœ¬çš„æè¿°æ€§ç»Ÿè®¡åˆ†æ
"""

import pandas as pd
import numpy as np
import warnings
from pathlib import Path

# è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 50)
warnings.filterwarnings('ignore')

def load_data():
    """åŠ è½½åº—é“ºä¿¡æ¯æ•°æ®"""
    data_path = Path("/Users/zihao_/Documents/coding/dataset/original/store_info_data.csv")
    
    if not data_path.exists():
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
    print("ğŸ“ æ­£åœ¨åŠ è½½åº—é“ºä¿¡æ¯æ•°æ®...")
    
    # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼å’Œåˆ†éš”ç¬¦
    encodings = ['utf-16', 'utf-16le', 'utf-8', 'gbk', 'gb2312']
    separators = ['\t', ',', ';', '|']
    
    for encoding in encodings:
        for sep in separators:
            try:
                print(f"   å°è¯•ä½¿ç”¨ {encoding} ç¼–ç ï¼Œåˆ†éš”ç¬¦: '{sep}'...")
                df = pd.read_csv(data_path, encoding=encoding, sep=sep)
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸè§£æï¼ˆåˆ—æ•°åº”è¯¥å¤§äº1ï¼‰
                if df.shape[1] > 1:
                    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼ä½¿ç”¨ç¼–ç : {encoding}ï¼Œåˆ†éš”ç¬¦: '{sep}'")
                    print(f"âœ… æ•°æ®å½¢çŠ¶: {df.shape}")
                    return df
                    
            except UnicodeDecodeError:
                continue
            except Exception as e:
                continue
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•æœ€åä¸€ç§æ–¹æ³•
    try:
        print("   å°è¯•æœ€åçš„æ–¹æ³•ï¼šutf-16ç¼–ç ï¼Œåˆ¶è¡¨ç¬¦åˆ†éš”...")
        df = pd.read_csv(data_path, encoding='utf-16', sep='\t', on_bad_lines='skip')
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼æ•°æ®å½¢çŠ¶: {df.shape}")
        return df
    except Exception as e:
        raise ValueError(f"æ— æ³•ä½¿ç”¨ä»»ä½•æ–¹å¼è¯»å–æ–‡ä»¶: {e}")

def basic_info_analysis(df):
    """åŸºæœ¬ä¿¡æ¯åˆ†æ"""
    print("\n" + "="*60)
    print("ğŸ“Š åŸºæœ¬ä¿¡æ¯åˆ†æ")
    print("="*60)
    
    print(f"ğŸ“ æ•°æ®ç»´åº¦: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
    print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    print("\nğŸ·ï¸ åˆ—åå’Œæ•°æ®ç±»å‹:")
    print("-" * 40)
    for i, (col, dtype) in enumerate(zip(df.columns, df.dtypes), 1):
        print(f"{i:2d}. {col:<30} {str(dtype)}")
    
    print(f"\nğŸ“‹ æ•°æ®é¢„è§ˆ (å‰5è¡Œ):")
    print("-" * 40)
    print(df.head())
    
    return df

def missing_values_analysis(df):
    """ç¼ºå¤±å€¼åˆ†æ"""
    print("\n" + "="*60)
    print("ğŸ” ç¼ºå¤±å€¼åˆ†æ")
    print("="*60)
    
    missing_stats = pd.DataFrame({
        'ç¼ºå¤±æ•°é‡': df.isnull().sum(),
        'ç¼ºå¤±æ¯”ä¾‹(%)': (df.isnull().sum() / len(df) * 100).round(2)
    })
    
    missing_stats = missing_stats[missing_stats['ç¼ºå¤±æ•°é‡'] > 0].sort_values('ç¼ºå¤±æ•°é‡', ascending=False)
    
    if len(missing_stats) > 0:
        print("å­˜åœ¨ç¼ºå¤±å€¼çš„åˆ—:")
        print(missing_stats)
    else:
        print("âœ… æ•°æ®ä¸­æ²¡æœ‰ç¼ºå¤±å€¼")

def numerical_analysis(df):
    """æ•°å€¼å‹å˜é‡åˆ†æ"""
    print("\n" + "="*60)
    print("ğŸ“ˆ æ•°å€¼å‹å˜é‡åˆ†æ")
    print("="*60)
    
    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numerical_cols) == 0:
        print("âŒ æ²¡æœ‰å‘ç°æ•°å€¼å‹å˜é‡")
        return
    
    print(f"ğŸ“Š å‘ç° {len(numerical_cols)} ä¸ªæ•°å€¼å‹å˜é‡:")
    for i, col in enumerate(numerical_cols, 1):
        print(f"{i:2d}. {col}")
    
    print(f"\nğŸ“‹ æ•°å€¼å‹å˜é‡æè¿°æ€§ç»Ÿè®¡:")
    print("-" * 40)
    desc_stats = df[numerical_cols].describe()
    print(desc_stats)

def categorical_analysis(df):
    """åˆ†ç±»å˜é‡åˆ†æ"""
    print("\n" + "="*60)
    print("ğŸ·ï¸ åˆ†ç±»å˜é‡åˆ†æ")
    print("="*60)
    
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    if len(categorical_cols) == 0:
        print("âŒ æ²¡æœ‰å‘ç°åˆ†ç±»å˜é‡")
        return
    
    print(f"ğŸ“Š å‘ç° {len(categorical_cols)} ä¸ªåˆ†ç±»å˜é‡:")
    for i, col in enumerate(categorical_cols, 1):
        print(f"{i:2d}. {col}")
    
    # åˆ†ææ¯ä¸ªåˆ†ç±»å˜é‡
    for col in categorical_cols[:10]:  # æœ€å¤šåˆ†æå‰10ä¸ªåˆ†ç±»å˜é‡
        print(f"\nğŸ“‹ {col} çš„å–å€¼åˆ†å¸ƒ:")
        print("-" * 40)
        
        value_counts = df[col].value_counts()
        print(f"å”¯ä¸€å€¼æ•°é‡: {df[col].nunique()}")
        print(f"æœ€é¢‘ç¹çš„å€¼: {value_counts.index[0]} (å‡ºç° {value_counts.iloc[0]} æ¬¡)")
        
        # æ˜¾ç¤ºå‰10ä¸ªæœ€é¢‘ç¹çš„å€¼
        print("\nå‰10ä¸ªæœ€é¢‘ç¹çš„å€¼:")
        top_values = value_counts.head(10)
        for value, count in top_values.items():
            percentage = (count / len(df)) * 100
            print(f"  {value}: {count} ({percentage:.1f}%)")

def data_quality_check(df):
    """æ•°æ®è´¨é‡æ£€æŸ¥"""
    print("\n" + "="*60)
    print("ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥")
    print("="*60)
    
    # æ£€æŸ¥é‡å¤è¡Œ
    duplicate_rows = df.duplicated().sum()
    print(f"ğŸ”„ é‡å¤è¡Œæ•°é‡: {duplicate_rows}")
    if duplicate_rows > 0:
        print(f"   é‡å¤æ¯”ä¾‹: {duplicate_rows/len(df)*100:.2f}%")
    
    # æ£€æŸ¥æ¯åˆ—çš„å”¯ä¸€å€¼æ•°é‡
    print(f"\nğŸ“Š å„åˆ—å”¯ä¸€å€¼ç»Ÿè®¡:")
    print("-" * 40)
    for col in df.columns:
        unique_count = df[col].nunique()
        unique_ratio = unique_count / len(df) * 100
        print(f"{col:<30} {unique_count:>8} ({unique_ratio:>5.1f}%)")

def generate_summary_report(df):
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“‹ æ•°æ®æ€»ç»“æŠ¥å‘Š")
    print("="*60)
    
    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    print(f"ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    print(f"  â€¢ æ€»è¡Œæ•°: {len(df):,}")
    print(f"  â€¢ æ€»åˆ—æ•°: {len(df.columns)}")
    print(f"  â€¢ æ•°å€¼å‹å˜é‡: {len(numerical_cols)} ä¸ª")
    print(f"  â€¢ åˆ†ç±»å˜é‡: {len(categorical_cols)} ä¸ª")
    print(f"  â€¢ ç¼ºå¤±å€¼æ€»æ•°: {df.isnull().sum().sum():,}")
    print(f"  â€¢ é‡å¤è¡Œæ•°: {df.duplicated().sum():,}")
    print(f"  â€¢ å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    print(f"\nğŸ¯ ä¸»è¦å‘ç°:")
    
    # ç¼ºå¤±å€¼æœ€å¤šçš„åˆ—
    missing_stats = df.isnull().sum().sort_values(ascending=False)
    if missing_stats.iloc[0] > 0:
        print(f"  â€¢ ç¼ºå¤±å€¼æœ€å¤šçš„åˆ—: {missing_stats.index[0]} ({missing_stats.iloc[0]} ä¸ª)")
    
    # å”¯ä¸€å€¼æœ€å¤šçš„åˆ—
    unique_stats = df.nunique().sort_values(ascending=False)
    print(f"  â€¢ å”¯ä¸€å€¼æœ€å¤šçš„åˆ—: {unique_stats.index[0]} ({unique_stats.iloc[0]} ä¸ª)")

def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸš€ å¼€å§‹åº—é“ºä¿¡æ¯æ•°æ®æ¢ç´¢åˆ†æ...")
        
        # 1. åŠ è½½æ•°æ®
        df = load_data()
        
        # 2. åŸºæœ¬ä¿¡æ¯åˆ†æ
        df = basic_info_analysis(df)
        
        # 3. ç¼ºå¤±å€¼åˆ†æ
        missing_values_analysis(df)
        
        # 4. æ•°å€¼å‹å˜é‡åˆ†æ
        numerical_analysis(df)
        
        # 5. åˆ†ç±»å˜é‡åˆ†æ
        categorical_analysis(df)
        
        # 6. æ•°æ®è´¨é‡æ£€æŸ¥
        data_quality_check(df)
        
        # 7. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        generate_summary_report(df)
        
        print(f"\nğŸ‰ åº—é“ºä¿¡æ¯æ•°æ®æ¢ç´¢åˆ†æå®Œæˆï¼")
        
        return df
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise

if __name__ == "__main__":
    df = main()