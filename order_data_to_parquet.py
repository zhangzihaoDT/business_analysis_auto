#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¢å•è§‚å¯Ÿæ•°æ®å¤„ç†è„šæœ¬

è¯¥è„šæœ¬ç”¨äºå¤„ç† order_observation_data.csv æ–‡ä»¶
å°†å…¶è½¬æ¢ä¸ºä¼˜åŒ–çš„Parquetæ ¼å¼

è¾“å…¥æ–‡ä»¶: original/order_observation_data.csv
è¾“å‡ºæ–‡ä»¶: formatted/order_observation_data.parquet
"""

import pandas as pd
import numpy as np
import chardet
import os
import json
import hashlib
from datetime import datetime
from pathlib import Path

def detect_encoding(file_path):
    """
    æ£€æµ‹æ–‡ä»¶ç¼–ç 
    """
    with open(file_path, 'rb') as f:
        raw_data = f.read(10000)  # è¯»å–å‰10000å­—èŠ‚è¿›è¡Œæ£€æµ‹
        result = chardet.detect(raw_data)
        return result['encoding'], result['confidence']

def read_csv_with_encoding(file_path):
    """
    ä½¿ç”¨å¤šç§ç¼–ç å°è¯•è¯»å–CSVæ–‡ä»¶
    """
    # é¦–å…ˆæ£€æµ‹æ–‡ä»¶ç¼–ç 
    encoding, confidence = detect_encoding(file_path)
    print(f"æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {encoding}ï¼Œç½®ä¿¡åº¦: {confidence:.2f}")
    
    # å°è¯•ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç è¯»å–
    try:
        df_data = pd.read_csv(file_path, encoding=encoding, sep='\t')
        print(f"ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å–æ–‡ä»¶")
        return df_data, encoding
    except Exception as e:
        print(f"ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç  {encoding} è¯»å–å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç ...")
        
        # å°è¯•å¸¸è§ç¼–ç 
        encodings_to_try = ['utf-16', 'utf-8', 'latin1', 'gbk', 'gb2312', 'gb18030']
        for enc in encodings_to_try:
            try:
                df_data = pd.read_csv(file_path, encoding=enc, sep='\t')
                print(f"ä½¿ç”¨ {enc} ç¼–ç æˆåŠŸè¯»å–æ–‡ä»¶")
                return df_data, enc
            except:
                continue
        
        raise Exception("å°è¯•äº†å¤šç§ç¼–ç ä½†éƒ½å¤±è´¥äº†")

def analyze_data_structure(df):
    """
    åˆ†ææ•°æ®ç»“æ„ï¼Œæ‰“å°å­—æ®µåç§°å’Œæ ¼å¼ä¿¡æ¯
    """
    print("\n" + "="*80)
    print(" æ•°æ®ç»“æ„åˆ†æ ")
    print("="*80)
    
    print(f"\nğŸ“Š æ•°æ®ç»´åº¦: {df.shape[0]} è¡Œ x {df.shape[1]} åˆ—")
    
    print(f"\nğŸ“‹ å­—æ®µåç§°åˆ—è¡¨:")
    for i, col in enumerate(df.columns, 1):
        print(f"{i:2d}. {col}")
    
    print(f"\nğŸ“ˆ å­—æ®µè¯¦ç»†ä¿¡æ¯:")
    print("-" * 80)
    print(f"{'åºå·':<4} {'å­—æ®µå':<35} {'æ•°æ®ç±»å‹':<15} {'éç©ºæ•°é‡':<10} {'ç©ºå€¼æ•°é‡':<10} {'ç©ºå€¼æ¯”ä¾‹':<10}")
    print("-" * 80)
    
    for i, col in enumerate(df.columns, 1):
        dtype = str(df[col].dtype)
        non_null_count = df[col].count()
        null_count = df[col].isnull().sum()
        null_percentage = (null_count / len(df)) * 100
        
        print(f"{i:<4} {col[:34]:<35} {dtype:<15} {non_null_count:<10} {null_count:<10} {null_percentage:<10.2f}%")
    
    print("\nğŸ“ æ•°æ®æ ·æœ¬ï¼ˆå‰5è¡Œï¼‰:")
    print("-" * 120)
    # æ˜¾ç¤ºå‰5è¡Œï¼Œä½†é™åˆ¶åˆ—å®½ä»¥ä¾¿æŸ¥çœ‹
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', 20)
    print(df.head())
    
    # é‡ç½®pandasæ˜¾ç¤ºé€‰é¡¹
    pd.reset_option('display.max_columns')
    pd.reset_option('display.width')
    pd.reset_option('display.max_colwidth')
    
    print("\nğŸ” æ•°å€¼å‹å­—æ®µç»Ÿè®¡:")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print(df[numeric_cols].describe())
    else:
        print("æœªå‘ç°æ•°å€¼å‹å­—æ®µ")
    
    print("\nğŸ“Š åˆ†ç±»å­—æ®µå”¯ä¸€å€¼ç»Ÿè®¡:")
    categorical_cols = df.select_dtypes(include=['object']).columns
    for col in categorical_cols[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªåˆ†ç±»å­—æ®µ
        unique_count = df[col].nunique()
        print(f"{col}: {unique_count} ä¸ªå”¯ä¸€å€¼")
        if unique_count <= 10:  # å¦‚æœå”¯ä¸€å€¼å°‘äºç­‰äº10ä¸ªï¼Œæ˜¾ç¤ºæ‰€æœ‰å€¼
            print(f"  å€¼: {df[col].unique().tolist()}")
        else:
            print(f"  å‰5ä¸ªå€¼: {df[col].value_counts().head().index.tolist()}")

def split_merged_columns(df_raw):
    """
    åˆ†å‰²åˆå¹¶çš„åˆ—ï¼ˆå¤„ç†åˆ¶è¡¨ç¬¦åˆ†éš”çš„æ•°æ®ï¼‰
    """
    if len(df_raw.columns) == 1:
        # è·å–ç¬¬ä¸€åˆ—çš„åç§°
        first_col_name = df_raw.columns[0]
        
        # åˆ†å‰²åˆ—å
        column_names = first_col_name.split('\t')
        print(f"åˆ†å‰²åçš„åˆ—å: {column_names}")
        
        # åˆ†å‰²æ•°æ®
        df_split = df_raw[first_col_name].str.split('\t', expand=True)
        
        # è®¾ç½®åˆ—å
        df_split.columns = column_names
        
        return df_split
    else:
        return df_raw

def clean_and_convert_data(df):
    """
    æ¸…ç†å’Œè½¬æ¢æ•°æ®ç±»å‹
    """
    print("\n" + "="*60)
    print(" å¼€å§‹æ•°æ®æ¸…æ´—å’Œç±»å‹è½¬æ¢ ")
    print("="*60)
    
    df_cleaned = df.copy()
    
    # 1. å¤„ç†æ—¥æœŸåˆ—
    date_columns = [
        'æ—¥(Intention Payment Time)',
        'æ—¥(Order Create Time)',
        'æ—¥(Lock Time)', 
        'æ—¥(intention_refund_time)',
        'æ—¥(Actual Refund Time)',
        'DATE([Invoice Upload Time])',
        'DATE([first_assign_time])'
    ]
    
    for col in date_columns:
        if col in df_cleaned.columns:
            try:
                # ä½¿ç”¨æ–°çš„ä¸­æ–‡æ—¥æœŸè§£æå‡½æ•°
                df_cleaned[col] = df_cleaned[col].apply(parse_chinese_date)
                
                # ç»Ÿè®¡è½¬æ¢ç»“æœ
                valid_dates = df_cleaned[col].notna().sum()
                total_rows = len(df_cleaned)
                success_rate = (valid_dates / total_rows) * 100
                
                print(f"âœ… æˆåŠŸå°† {col} è½¬æ¢ä¸ºæ—¥æœŸç±»å‹ (æœ‰æ•ˆæ—¥æœŸ: {valid_dates}/{total_rows}, {success_rate:.2f}%)")
            except Exception as e:
                print(f"âŒ è½¬æ¢ {col} æ—¶å‡ºé”™: {e}")
    
    # 2. å¤„ç†æ•°å€¼åˆ—
    numeric_columns = ['buyer_age', 'å¹³å‡å€¼ Origin Amount', 'å¹³å‡å€¼ å¼€ç¥¨ä»·æ ¼', 'å¹³å‡å€¼ æŠ˜æ‰£ç‡', 'Order Number ä¸åŒè®¡æ•°']
    for col in numeric_columns:
        if col in df_cleaned.columns:
            try:
                df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')
                print(f"âœ… æˆåŠŸå°† {col} è½¬æ¢ä¸ºæ•°å€¼ç±»å‹")
            except Exception as e:
                print(f"âŒ è½¬æ¢ {col} æ—¶å‡ºé”™: {e}")
    
    # 3. å¤„ç†åˆ†ç±»å˜é‡
    category_columns = [
        'è½¦å‹åˆ†ç»„', 'pre_vehicle_model_type', 'Product Name', 
        'sales_loyalty_type', 'order_gender', 'Province Name', 
        'License City', 'license_city_level', 'Parent Region Name',
        'first_middle_channel_name'
    ]
    
    for col in category_columns:
        if col in df_cleaned.columns:
            try:
                # æ£€æŸ¥å”¯ä¸€å€¼æ¯”ä¾‹ï¼Œå¦‚æœå°äº50%åˆ™è½¬æ¢ä¸ºcategory
                unique_ratio = df_cleaned[col].nunique() / len(df_cleaned)
                if unique_ratio < 0.5:
                    df_cleaned[col] = df_cleaned[col].astype('category')
                    print(f"âœ… å·²å°† {col} è½¬æ¢ä¸ºcategoryç±»å‹ (å”¯ä¸€å€¼æ¯”ä¾‹: {unique_ratio:.2%})")
                else:
                    print(f"âš ï¸  {col} å”¯ä¸€å€¼æ¯”ä¾‹è¿‡é«˜ ({unique_ratio:.2%})ï¼Œä¿æŒä¸ºobjectç±»å‹")
            except Exception as e:
                print(f"âŒ è½¬æ¢ {col} æ—¶å‡ºé”™: {e}")
    
    return df_cleaned

def standardize_columns(df):
    """
    æ ‡å‡†åŒ–åˆ—åå’Œæ•°æ®ç»“æ„
    """
    print("\n" + "="*60)
    print(" å¼€å§‹åˆ—åæ ‡å‡†åŒ– ")
    print("="*60)
    
    df_standardized = df.copy()
    
    # æ‰“å°å½“å‰åˆ—å
    print(f"å½“å‰æ•°æ®åˆ—æ•°: {len(df_standardized.columns)}")
    print("å½“å‰åˆ—å:")
    for i, col in enumerate(df_standardized.columns, 1):
        print(f"{i:2d}. {col}")
    
    return df_standardized

def optimize_data_types(df):
    """
    ä¼˜åŒ–æ•°æ®ç±»å‹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
    """
    print("\n" + "="*60)
    print(" å¼€å§‹æ•°æ®ç±»å‹ä¼˜åŒ– ")
    print("="*60)
    
    df_optimized = df.copy()
    
    # å¯¹äºæ•´æ•°åˆ—ï¼Œå°è¯•ä½¿ç”¨æ›´å°çš„æ•°æ®ç±»å‹
    int_columns = ['buyer_age', 'åº¦é‡å€¼']
    for col in int_columns:
        if col in df_optimized.columns and df_optimized[col].dtype in ['float64', 'int64']:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰éç©ºå€¼
                if df_optimized[col].notna().any():
                    non_null_data = df_optimized[col].dropna()
                    if len(non_null_data) == 0:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å°æ•°éƒ¨åˆ†
                    has_decimals = (non_null_data % 1 != 0).any()
                    if has_decimals:
                        print(f"âš ï¸  {col} åŒ…å«å°æ•°å€¼ï¼Œä¿æŒä¸ºfloat64ç±»å‹")
                        continue
                        
                    min_val = non_null_data.min()
                    max_val = non_null_data.max()
                    
                    # æ ¹æ®æ•°æ®èŒƒå›´é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹ï¼ˆä½¿ç”¨å¯ç©ºæ•´æ•°ç±»å‹ï¼‰
                    if min_val >= 0 and max_val <= 255:
                        df_optimized[col] = df_optimized[col].astype('UInt8')
                    elif min_val >= -128 and max_val <= 127:
                        df_optimized[col] = df_optimized[col].astype('Int8')
                    elif min_val >= 0 and max_val <= 65535:
                        df_optimized[col] = df_optimized[col].astype('UInt16')
                    elif min_val >= -32768 and max_val <= 32767:
                        df_optimized[col] = df_optimized[col].astype('Int16')
                    elif min_val >= 0 and max_val <= 4294967295:
                        df_optimized[col] = df_optimized[col].astype('UInt32')
                    elif min_val >= -2147483648 and max_val <= 2147483647:
                        df_optimized[col] = df_optimized[col].astype('Int32')
                    else:
                        df_optimized[col] = df_optimized[col].astype('Int64')
                    
                    print(f"âœ… å·²ä¼˜åŒ– {col} çš„æ•°æ®ç±»å‹ä¸º: {df_optimized[col].dtype}")
            except Exception as e:
                print(f"âŒ ä¼˜åŒ– {col} æ•°æ®ç±»å‹æ—¶å‡ºé”™: {e}ï¼Œä¿æŒåŸç±»å‹")
    
    return df_optimized

def parse_chinese_date(date_str):
    """è§£æä¸­æ–‡æ—¥æœŸæ ¼å¼ï¼Œå¦‚'2025å¹´8æœˆ25æ—¥'"""
    if pd.isna(date_str) or date_str == 'nan':
        return pd.NaT
    try:
        # å¤„ç†ä¸­æ–‡æ—¥æœŸæ ¼å¼
        if 'å¹´' in str(date_str) and 'æœˆ' in str(date_str) and 'æ—¥' in str(date_str):
            date_str = str(date_str).replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '')
            return pd.to_datetime(date_str)
        else:
            return pd.to_datetime(date_str, errors='coerce')
    except:
        return pd.NaT

def get_file_hash(file_path):
    """
    è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼
    """
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        print(f"âŒ è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼å¤±è´¥: {e}")
        return None

def load_file_tracking_info(tracking_file_path):
    """
    åŠ è½½æ–‡ä»¶è·Ÿè¸ªä¿¡æ¯
    """
    if os.path.exists(tracking_file_path):
        try:
            with open(tracking_file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ–‡ä»¶è·Ÿè¸ªä¿¡æ¯å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°çš„è·Ÿè¸ªæ–‡ä»¶")
    return {}

def save_file_tracking_info(tracking_file_path, tracking_info):
    """
    ä¿å­˜æ–‡ä»¶è·Ÿè¸ªä¿¡æ¯
    """
    try:
        with open(tracking_file_path, 'w', encoding='utf-8') as f:
            json.dump(tracking_info, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ–‡ä»¶è·Ÿè¸ªä¿¡æ¯å·²ä¿å­˜åˆ°: {tracking_file_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶è·Ÿè¸ªä¿¡æ¯å¤±è´¥: {e}")

def check_file_updates(input_dir, tracking_file_path):
    """
    æ£€æŸ¥æ–‡ä»¶æ›´æ–°ï¼Œè¿”å›éœ€è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    """
    print("\n" + "="*60)
    print(" æ£€æŸ¥æ–‡ä»¶æ›´æ–° ")
    print("="*60)
    
    # åŠ è½½ç°æœ‰çš„è·Ÿè¸ªä¿¡æ¯
    tracking_info = load_file_tracking_info(tracking_file_path)
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = list(Path(input_dir).glob("*.csv"))
    files_to_process = []
    
    for csv_file in csv_files:
        file_path = str(csv_file)
        file_name = csv_file.name
        
        # è®¡ç®—å½“å‰æ–‡ä»¶å“ˆå¸Œå€¼
        current_hash = get_file_hash(file_path)
        if current_hash is None:
            continue
            
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
        if file_name not in tracking_info:
            print(f"ğŸ†• å‘ç°æ–°æ–‡ä»¶: {file_name}")
            files_to_process.append(file_path)
        elif tracking_info[file_name].get('hash') != current_hash:
            print(f"ğŸ”„ æ–‡ä»¶å·²æ›´æ–°: {file_name}")
            files_to_process.append(file_path)
        else:
            print(f"âœ… æ–‡ä»¶æ— å˜åŒ–: {file_name}")
    
    if not files_to_process:
        print("ğŸ“‹ æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€å¤„ç†")
    else:
        print(f"ğŸ“Š éœ€è¦å¤„ç† {len(files_to_process)} ä¸ªæ–‡ä»¶")
    
    return files_to_process, tracking_info

def process_single_file(file_path, output_dir):
    """
    å¤„ç†å•ä¸ªCSVæ–‡ä»¶å¹¶è½¬æ¢ä¸ºParquetæ ¼å¼
    """
    file_name = Path(file_path).stem
    print(f"\nğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶: {file_name}")
    
    try:
        # 1. è¯»å–CSVæ–‡ä»¶
        df_raw, encoding = read_csv_with_encoding(file_path)
        print(f"åŸå§‹æ•°æ®ç»´åº¦: {df_raw.shape}")
        
        # 2. åˆ†å‰²åˆå¹¶çš„åˆ—
        df_data = split_merged_columns(df_raw)
        print(f"åˆ†å‰²åæ•°æ®ç»´åº¦: {df_data.shape}")
        
        # 3. æ•°æ®é€è§†å¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if 'åº¦é‡åç§°' in df_data.columns and 'åº¦é‡å€¼' in df_data.columns:
            df_pivoted = pivot_metrics_data(df_data)
        else:
            print("âš ï¸  æœªå‘ç°åº¦é‡åç§°å’Œåº¦é‡å€¼åˆ—ï¼Œè·³è¿‡é€è§†å¤„ç†")
            df_pivoted = df_data
        
        # 4. æ ‡å‡†åŒ–åˆ—å
        df_standardized = standardize_columns(df_pivoted)
        
        # 5. æ¸…ç†å’Œè½¬æ¢æ•°æ®ç±»å‹
        df_cleaned = clean_and_convert_data(df_standardized)
        
        # 6. ä¼˜åŒ–æ•°æ®ç±»å‹
        df_optimized = optimize_data_types(df_cleaned)
        
        # 7. ä¿å­˜ä¸ºParquetæ–‡ä»¶
        output_file_path = os.path.join(output_dir, f"{file_name}.parquet")
        df_optimized.to_parquet(output_file_path, index=False, engine='pyarrow', compression='snappy')
        
        # 8. è®¡ç®—æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(output_file_path) / (1024 * 1024)  # MB
        
        print(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {file_name}")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file_path}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        print(f"ğŸ“ˆ æ•°æ®ç»´åº¦: {df_optimized.shape[0]} è¡Œ x {df_optimized.shape[1]} åˆ—")
        
        return df_optimized, output_file_path
        
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None, None

def merge_parquet_files(parquet_files, output_file_path):
    """
    åˆå¹¶å¤šä¸ªParquetæ–‡ä»¶ä¸ºä¸€ä¸ªå®Œæ•´çš„æ–‡ä»¶
    """
    print("\n" + "="*60)
    print(" åˆå¹¶Parquetæ–‡ä»¶ ")
    print("="*60)
    
    if not parquet_files:
        print("âš ï¸  æ²¡æœ‰Parquetæ–‡ä»¶éœ€è¦åˆå¹¶")
        return None
    
    try:
        # è¯»å–æ‰€æœ‰Parquetæ–‡ä»¶
        dataframes = []
        total_rows = 0
        
        for parquet_file in parquet_files:
            if os.path.exists(parquet_file):
                df = pd.read_parquet(parquet_file)
                dataframes.append(df)
                total_rows += len(df)
                print(f"ğŸ“ å·²åŠ è½½: {Path(parquet_file).name} ({len(df)} è¡Œ)")
            else:
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {parquet_file}")
        
        if not dataframes:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„Parquetæ–‡ä»¶å¯ä»¥åˆå¹¶")
            return None
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†
        print(f"ğŸ”„ æ­£åœ¨åˆå¹¶ {len(dataframes)} ä¸ªæ•°æ®æ–‡ä»¶...")
        merged_df = pd.concat(dataframes, ignore_index=True)
        
        # å»é‡å¤„ç†ï¼ˆåŸºäºOrder Numberï¼‰
        if 'Order Number' in merged_df.columns:
            original_count = len(merged_df)
            merged_df = merged_df.drop_duplicates(subset=['Order Number'], keep='last')
            deduplicated_count = len(merged_df)
            removed_count = original_count - deduplicated_count
            
            if removed_count > 0:
                print(f"ğŸ”„ å·²å»é™¤ {removed_count} ä¸ªé‡å¤è®¢å•")
            print(f"ğŸ“Š åˆå¹¶åæ•°æ®ç»´åº¦: {merged_df.shape[0]} è¡Œ x {merged_df.shape[1]} åˆ—")
        
        # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
        merged_df.to_parquet(output_file_path, index=False, engine='pyarrow', compression='snappy')
        
        # è®¡ç®—æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(output_file_path) / (1024 * 1024)  # MB
        
        print(f"âœ… æ–‡ä»¶åˆå¹¶å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file_path}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        print(f"ğŸ“ˆ æœ€ç»ˆæ•°æ®ç»´åº¦: {merged_df.shape[0]} è¡Œ x {merged_df.shape[1]} åˆ—")
        
        return merged_df
        
    except Exception as e:
        print(f"âŒ åˆå¹¶æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def process_order_data_to_parquet(input_dir=None, output_dir=None):
    """
    æ‰¹é‡å¤„ç†è®¢å•è§‚å¯Ÿæ•°æ®å¹¶è½¬æ¢ä¸ºParquetæ ¼å¼
    
    Args:
        input_dir (str): è¾“å…¥ç›®å½•è·¯å¾„ï¼ŒåŒ…å«å¤šä¸ªCSVæ–‡ä»¶
        output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
    
    Returns:
        tuple: (åˆå¹¶åçš„DataFrame, æœ€ç»ˆè¾“å‡ºæ–‡ä»¶è·¯å¾„)
    """
    # é»˜è®¤è·¯å¾„è®¾ç½®
    if input_dir is None:
        input_dir = "/Users/zihao_/Documents/coding/dataset/original/order_observation_data/"
    
    if output_dir is None:
        output_dir = "/Users/zihao_/Documents/coding/dataset/formatted/"
    
    # æ–‡ä»¶è·Ÿè¸ªä¿¡æ¯è·¯å¾„
    tracking_file_path = os.path.join(output_dir, "file_tracking.json")
    
    # æœ€ç»ˆåˆå¹¶æ–‡ä»¶è·¯å¾„
    final_output_file_path = os.path.join(output_dir, "order_observation_data_merged.parquet")
    
    try:
        print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†è®¢å•è§‚å¯Ÿæ•°æ®...")
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # 1. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        
        # 2. æ£€æŸ¥æ–‡ä»¶æ›´æ–°
        files_to_process, tracking_info = check_file_updates(input_dir, tracking_file_path)
        
        if not files_to_process:
            print("\nğŸ“‹ æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€å¤„ç†")
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœ€ç»ˆåˆå¹¶æ–‡ä»¶
            if os.path.exists(final_output_file_path):
                print(f"âœ… æœ€ç»ˆåˆå¹¶æ–‡ä»¶å·²å­˜åœ¨: {final_output_file_path}")
                return pd.read_parquet(final_output_file_path), final_output_file_path
            else:
                print("âš ï¸  æœ€ç»ˆåˆå¹¶æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†é‡æ–°åˆå¹¶ç°æœ‰çš„Parquetæ–‡ä»¶")
        
        # 3. å¤„ç†éœ€è¦æ›´æ–°çš„æ–‡ä»¶
        processed_files = []
        for file_path in files_to_process:
            df_processed, output_file_path = process_single_file(file_path, output_dir)
            if df_processed is not None and output_file_path is not None:
                processed_files.append(output_file_path)
                
                # æ›´æ–°è·Ÿè¸ªä¿¡æ¯
                file_name = Path(file_path).name
                file_hash = get_file_hash(file_path)
                tracking_info[file_name] = {
                    'hash': file_hash,
                    'processed_time': datetime.now().isoformat(),
                    'output_file': output_file_path
                }
        
        # 4. ä¿å­˜è·Ÿè¸ªä¿¡æ¯
        if processed_files:
            save_file_tracking_info(tracking_file_path, tracking_info)
        
        # 5. è·å–æ‰€æœ‰ç°æœ‰çš„Parquetæ–‡ä»¶è¿›è¡Œåˆå¹¶
        all_parquet_files = []
        for file_info in tracking_info.values():
            output_file = file_info.get('output_file')
            if output_file and os.path.exists(output_file):
                all_parquet_files.append(output_file)
        
        # 6. åˆå¹¶æ‰€æœ‰Parquetæ–‡ä»¶
        if all_parquet_files:
            merged_df = merge_parquet_files(all_parquet_files, final_output_file_path)
            
            if merged_df is not None:
                # 7. æ•°æ®è´¨é‡æ£€æŸ¥
                print("\n" + "="*60)
                print(" æœ€ç»ˆæ•°æ®è´¨é‡æŠ¥å‘Š ")
                print("="*60)
                print(f"æœ€ç»ˆæ•°æ®ç»´åº¦: {merged_df.shape}")
                print(f"\nå„åˆ—æ•°æ®ç±»å‹:")
                print(merged_df.dtypes)
                
                print(f"\nå„åˆ—ç©ºå€¼æ•°é‡:")
                null_counts = merged_df.isnull().sum()
                for col, count in null_counts.items():
                    if count > 0:
                        percentage = (count / len(merged_df)) * 100
                        print(f"{col}: {count} ({percentage:.2f}%)")
                
                # 8. æ˜¾ç¤ºæœ€ç»ˆæ•°æ®æ ·æœ¬
                print(f"\næœ€ç»ˆæ•°æ®æ ·æœ¬ï¼ˆå‰5è¡Œï¼‰:")
                print(merged_df.head())
                
                return merged_df, final_output_file_path
            else:
                print("âŒ æ–‡ä»¶åˆå¹¶å¤±è´¥")
                return None, None
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯åˆå¹¶çš„Parquetæ–‡ä»¶")
            return None, None
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise e

if __name__ == "__main__":
    # æ‰§è¡Œæ•°æ®å¤„ç†
    try:
        df, output_path = process_order_data_to_parquet()
        print("\nğŸ‰ è®¢å•è§‚å¯Ÿæ•°æ®å¤„ç†å®Œæˆï¼")
    except Exception as e:
        print(f"\nğŸ’¥ å¤„ç†å¤±è´¥: {e}")