#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çº¿ç´¢ç»“æ„åˆ†ææ•°æ®å¤„ç†è„šæœ¬

è¯¥è„šæœ¬ç”¨äºå¤„ç† leads_structure_analysis.xlsx æ–‡ä»¶
å°†å…¶è½¬æ¢ä¸ºä¼˜åŒ–çš„Parquetæ ¼å¼

è¾“å…¥æ–‡ä»¶: original/leads_structure_analysis.xlsx
è¾“å‡ºæ–‡ä»¶: formatted/leads_structure_analysis.parquet
"""

import pandas as pd
import numpy as np
import os
import chardet
from datetime import datetime

def detect_encoding(file_path):
    """
    æ£€æµ‹æ–‡ä»¶ç¼–ç 
    """
    with open(file_path, 'rb') as f:
        raw_data = f.read(10000)  # è¯»å–å‰10000å­—èŠ‚è¿›è¡Œæ£€æµ‹
        result = chardet.detect(raw_data)
        return result['encoding'], result['confidence']

def read_excel_file(file_path):
    """
    è¯»å–Excelæ–‡ä»¶
    å¤„ç†leads_structure_analysis.xlsxæ–‡ä»¶
    """
    try:
        # è¯»å–Excelæ–‡ä»¶ï¼Œå°è¯•ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
        df_data = pd.read_excel(file_path, sheet_name=0)
        print(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶")
        print(f"åˆ—å: {list(df_data.columns)}")
        return df_data
    except Exception as e:
        print(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
        
        # å°è¯•è¯»å–æ‰€æœ‰å·¥ä½œè¡¨ï¼Œçœ‹çœ‹æœ‰å“ªäº›å¯ç”¨
        try:
            excel_file = pd.ExcelFile(file_path)
            print(f"Excelæ–‡ä»¶åŒ…å«çš„å·¥ä½œè¡¨: {excel_file.sheet_names}")
            
            # å°è¯•è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
            if len(excel_file.sheet_names) > 0:
                df_data = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])
                print(f"æˆåŠŸè¯»å–å·¥ä½œè¡¨: {excel_file.sheet_names[0]}")
                return df_data
        except Exception as e2:
            print(f"å°è¯•è¯»å–å·¥ä½œè¡¨ä¹Ÿå¤±è´¥: {e2}")
        
        raise Exception("æ— æ³•è¯»å–Excelæ–‡ä»¶")

def analyze_data_structure(df):
    """
    åˆ†ææ•°æ®ç»“æ„ï¼Œæ‰“å°å­—æ®µåç§°å’Œæ ¼å¼ä¿¡æ¯
    """
    print("\n" + "="*80)
    print(" æ•°æ®ç»“æ„åˆ†æ ")
    print("="*80)
    
    print(f"\nğŸ“Š æ•°æ®ç»´åº¦: {df.shape[0]} è¡Œ x {df.shape[1]} åˆ—")
    
    print(f"\nğŸ“‹ å­—æ®µåç§°åˆ—è¡¨:")
    for i, col in enumerate(df.columns, 1):
        print(f"{i:2d}. {col}")
    
    print(f"\nğŸ“ˆ å­—æ®µè¯¦ç»†ä¿¡æ¯:")
    print("-" * 80)
    print(f"{'åºå·':<4} {'å­—æ®µå':<35} {'æ•°æ®ç±»å‹':<15} {'éç©ºæ•°é‡':<10} {'ç©ºå€¼æ•°é‡':<10} {'ç©ºå€¼æ¯”ä¾‹':<10}")
    print("-" * 80)
    
    for i, col in enumerate(df.columns, 1):
        dtype = str(df[col].dtype)
        non_null_count = df[col].count()
        null_count = df[col].isnull().sum()
        null_percentage = (null_count / len(df)) * 100
        
        print(f"{i:<4} {col[:34]:<35} {dtype:<15} {non_null_count:<10} {null_count:<10} {null_percentage:<10.2f}%")
    
    print("\nğŸ“ æ•°æ®æ ·æœ¬ï¼ˆå‰5è¡Œï¼‰:")
    print("-" * 120)
    # æ˜¾ç¤ºå‰5è¡Œï¼Œä½†é™åˆ¶åˆ—å®½ä»¥ä¾¿æŸ¥çœ‹
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', 20)
    print(df.head())
    
    # é‡ç½®pandasæ˜¾ç¤ºé€‰é¡¹
    pd.reset_option('display.max_columns')
    pd.reset_option('display.width')
    pd.reset_option('display.max_colwidth')
    
    print("\nğŸ” æ•°å€¼å‹å­—æ®µç»Ÿè®¡:")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print(df[numeric_cols].describe())
    else:
        print("æœªå‘ç°æ•°å€¼å‹å­—æ®µ")
    
    print("\nğŸ“Š åˆ†ç±»å­—æ®µå”¯ä¸€å€¼ç»Ÿè®¡:")
    categorical_cols = df.select_dtypes(include=['object']).columns
    for col in categorical_cols[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªåˆ†ç±»å­—æ®µ
        unique_count = df[col].nunique()
        print(f"{col}: {unique_count} ä¸ªå”¯ä¸€å€¼")
        if unique_count <= 10:  # å¦‚æœå”¯ä¸€å€¼å°‘äºç­‰äº10ä¸ªï¼Œæ˜¾ç¤ºæ‰€æœ‰å€¼
            print(f"  å€¼: {df[col].unique().tolist()}")
        else:
            print(f"  å‰5ä¸ªå€¼: {df[col].value_counts().head().index.tolist()}")

def clean_and_convert_data(df):
    """
    æ¸…ç†å’Œè½¬æ¢æ•°æ®ç±»å‹
    """
    print("\n" + "="*60)
    print(" å¼€å§‹æ•°æ®æ¸…æ´—å’Œç±»å‹è½¬æ¢ ")
    print("="*60)
    
    df_cleaned = df.copy()
    
    # 1. å¤„ç†æ—¥æœŸåˆ—
    date_column = 'æ—¥(lc_create_time)'
    if date_column in df_cleaned.columns:
        try:
            # å¤„ç†ä¸­æ–‡æ—¥æœŸæ ¼å¼ï¼ˆå¦‚ï¼š2023å¹´8æœˆ24æ—¥ï¼‰
            # å…ˆå°†ä¸­æ–‡æ—¥æœŸè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            def convert_chinese_date(date_str):
                if pd.isna(date_str) or date_str == '':
                    return None
                try:
                    # ç§»é™¤'å¹´'ã€'æœˆ'ã€'æ—¥'å­—ç¬¦ï¼Œå¹¶æ›¿æ¢ä¸ºæ ‡å‡†åˆ†éš”ç¬¦
                    date_str = str(date_str).replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '')
                    return pd.to_datetime(date_str, format='%Y-%m-%d')
                except:
                    return None
            
            df_cleaned[date_column] = df_cleaned[date_column].apply(convert_chinese_date)
            print(f"âœ… æˆåŠŸå°† {date_column} è½¬æ¢ä¸ºæ—¥æœŸç±»å‹ï¼ˆå¤„ç†ä¸­æ–‡æ ¼å¼ï¼‰")
            
            # æ˜¾ç¤ºè½¬æ¢åçš„æ—¥æœŸæ ·æœ¬
            valid_dates = df_cleaned[date_column].dropna()
            if len(valid_dates) > 0:
                print(f"   è½¬æ¢åçš„æ—¥æœŸæ ·æœ¬: {valid_dates.head().tolist()}")
            else:
                print(f"   âš ï¸ è­¦å‘Š: æ²¡æœ‰æˆåŠŸè½¬æ¢çš„æ—¥æœŸ")
                
        except Exception as e:
            print(f"âŒ è½¬æ¢ {date_column} æ—¶å‡ºé”™: {e}")
    
    # 2. å¤„ç†æ•°å€¼åˆ—ï¼ˆé™¤äº†æ—¥æœŸåˆ—å¤–çš„æ‰€æœ‰åˆ—éƒ½åº”è¯¥æ˜¯æ•°å€¼å‹ï¼‰
    numeric_columns = [col for col in df_cleaned.columns if col != date_column]
    
    for col in numeric_columns:
        if col in df_cleaned.columns:
            try:
                # å¤„ç†åŒ…å«é€—å·çš„æ•°å­—ï¼ˆå¦‚ "4,399"ï¼‰
                if df_cleaned[col].dtype == 'object':
                    # ç§»é™¤é€—å·å¹¶è½¬æ¢ä¸ºæ•°å€¼
                    df_cleaned[col] = df_cleaned[col].astype(str).str.replace(',', '').replace('', '0')
                
                df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')
                print(f"âœ… æˆåŠŸå°† {col} è½¬æ¢ä¸ºæ•°å€¼ç±»å‹")
            except Exception as e:
                print(f"âŒ è½¬æ¢ {col} æ—¶å‡ºé”™: {e}")
    
    return df_cleaned

def optimize_data_types(df):
    """
    ä¼˜åŒ–æ•°æ®ç±»å‹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
    """
    print("\n" + "="*60)
    print(" å¼€å§‹æ•°æ®ç±»å‹ä¼˜åŒ– ")
    print("="*60)
    
    df_optimized = df.copy()
    
    # å¯¹äºæ•°å€¼åˆ—ï¼Œå°è¯•ä½¿ç”¨æ›´å°çš„æ•°æ®ç±»å‹
    for col in df_optimized.columns:
        if col != 'æ—¥(lc_create_time)' and df_optimized[col].dtype in ['float64', 'int64']:
            # æ£€æŸ¥æ˜¯å¦æœ‰éç©ºå€¼
            if df_optimized[col].notna().any():
                non_null_data = df_optimized[col].dropna()
                if len(non_null_data) == 0:
                    continue
                    
                min_val = non_null_data.min()
                max_val = non_null_data.max()
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å°æ•°ï¼ˆæ¯”ä¾‹å­—æ®µï¼‰
                has_decimals = (non_null_data % 1 != 0).any()
                
                if has_decimals or 'æ¯”ä¾‹' in col or 'ç‡' in col:
                    # å¯¹äºæ¯”ä¾‹æˆ–åŒ…å«å°æ•°çš„å­—æ®µï¼Œä½¿ç”¨float32
                    df_optimized[col] = df_optimized[col].astype('float32')
                    print(f"âœ… å·²ä¼˜åŒ– {col} çš„æ•°æ®ç±»å‹ä¸º: {df_optimized[col].dtype} (ä¿æŒæµ®ç‚¹æ•°)")
                else:
                    # å¯¹äºæ•´æ•°å­—æ®µï¼Œæ ¹æ®æ•°æ®èŒƒå›´é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹ï¼ˆä½¿ç”¨å¯ç©ºæ•´æ•°ç±»å‹ï¼‰
                    if min_val >= 0 and max_val <= 255:
                        df_optimized[col] = df_optimized[col].astype('UInt8')
                    elif min_val >= -128 and max_val <= 127:
                        df_optimized[col] = df_optimized[col].astype('Int8')
                    elif min_val >= 0 and max_val <= 65535:
                        df_optimized[col] = df_optimized[col].astype('UInt16')
                    elif min_val >= -32768 and max_val <= 32767:
                        df_optimized[col] = df_optimized[col].astype('Int16')
                    elif min_val >= 0 and max_val <= 4294967295:
                        df_optimized[col] = df_optimized[col].astype('UInt32')
                    elif min_val >= -2147483648 and max_val <= 2147483647:
                        df_optimized[col] = df_optimized[col].astype('Int32')
                    else:
                        df_optimized[col] = df_optimized[col].astype('Int64')
                    
                    print(f"âœ… å·²ä¼˜åŒ– {col} çš„æ•°æ®ç±»å‹ä¸º: {df_optimized[col].dtype}")
    
    return df_optimized

def process_leads_structure_analysis_to_parquet():
    """
    å¤„ç†çº¿ç´¢ç»“æ„åˆ†ææ•°æ®å¹¶è½¬æ¢ä¸ºParquetæ ¼å¼
    """
    # å®šä¹‰æ–‡ä»¶è·¯å¾„
    input_file_path = "/Users/zihao_/Documents/coding/dataset/original/leads_structure_analysis.xlsx"
    output_dir = "/Users/zihao_/Documents/coding/dataset/formatted/"
    output_file_path = os.path.join(output_dir, "leads_structure_analysis.parquet")
    
    try:
        print("ğŸš€ å¼€å§‹å¤„ç†çº¿ç´¢ç»“æ„åˆ†ææ•°æ®...")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # 1. è¯»å–Excelæ–‡ä»¶
        df_raw = read_excel_file(input_file_path)
        print(f"\nåŸå§‹æ•°æ®ç»´åº¦: {df_raw.shape}")
        
        # 2. åˆ†ææ•°æ®ç»“æ„
        analyze_data_structure(df_raw)
        
        # 3. æ¸…ç†å’Œè½¬æ¢æ•°æ®ç±»å‹
        df_cleaned = clean_and_convert_data(df_raw)
        
        # 4. ä¼˜åŒ–æ•°æ®ç±»å‹
        df_optimized = optimize_data_types(df_cleaned)
        
        # 5. æ•°æ®è´¨é‡æ£€æŸ¥
        print("\n" + "="*60)
        print(" æ•°æ®è´¨é‡æŠ¥å‘Š ")
        print("="*60)
        print(f"æœ€ç»ˆæ•°æ®ç»´åº¦: {df_optimized.shape}")
        print(f"\nå„åˆ—æ•°æ®ç±»å‹:")
        print(df_optimized.dtypes)
        
        print(f"\nå„åˆ—ç©ºå€¼æ•°é‡:")
        null_counts = df_optimized.isnull().sum()
        for col, count in null_counts.items():
            if count > 0:
                percentage = (count / len(df_optimized)) * 100
                print(f"{col}: {count} ({percentage:.2f}%)")
        
        # 6. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        
        # 7. ä¿å­˜ä¸ºParquetæ–‡ä»¶
        df_optimized.to_parquet(output_file_path, index=False, engine='pyarrow', compression='snappy')
        
        # 8. è®¡ç®—æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(output_file_path) / (1024 * 1024)  # MB
        
        print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file_path}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        print(f"ğŸ“ˆ æ•°æ®ç»´åº¦: {df_optimized.shape[0]} è¡Œ x {df_optimized.shape[1]} åˆ—")
        
        # 9. æ˜¾ç¤ºæœ€ç»ˆæ•°æ®æ ·æœ¬
        print(f"\næœ€ç»ˆæ•°æ®æ ·æœ¬ï¼ˆå‰5è¡Œï¼‰:")
        print(df_optimized.head())
        
        return df_optimized, output_file_path
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise e

if __name__ == "__main__":
    # æ‰§è¡Œæ•°æ®å¤„ç†
    try:
        df, output_path = process_leads_structure_analysis_to_parquet()
        print("\nğŸ‰ çº¿ç´¢ç»“æ„åˆ†ææ•°æ®å¤„ç†å®Œæˆï¼")
    except Exception as e:
        print(f"\nğŸ’¥ å¤„ç†å¤±è´¥: {e}")