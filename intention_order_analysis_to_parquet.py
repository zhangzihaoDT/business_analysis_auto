#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ„å‘è®¢å•åˆ†ææ•°æ®å¤„ç†è„šæœ¬

è¯¥è„šæœ¬ç”¨äºå¤„ç† Intention_Order_Analysis_(Series_6)_data.csv æ–‡ä»¶
å°†å…¶è½¬æ¢ä¸ºä¼˜åŒ–çš„Parquetæ ¼å¼

è¾“å…¥æ–‡ä»¶: original/Intention_Order_Analysis_(Series_6)_data.csv
è¾“å‡ºæ–‡ä»¶: formatted/intention_order_analysis.parquet
"""

import pandas as pd
import numpy as np
import os
import chardet
import json
from datetime import datetime
from pathlib import Path

def load_processing_metadata(metadata_path):
    """
    åŠ è½½å¤„ç†å…ƒæ•°æ®
    """
    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸  è¯»å–å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°çš„å…ƒæ•°æ®")
    
    # è¿”å›é»˜è®¤å…ƒæ•°æ®
    return {
        "last_processed_timestamp": None,
        "last_csv_modification_time": None,
        "last_processing_time": None,
        "total_records_processed": 0,
        "processing_history": [],
        "data_version": "1.0.0",
        "schema_version": "1.0.0",
        "incremental_mode": True,
        "notes": "Metadata file for tracking incremental updates of intention_order_analysis data"
    }

def save_processing_metadata(metadata_path, metadata):
    """
    ä¿å­˜å¤„ç†å…ƒæ•°æ®
    """
    try:
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        return False

def check_csv_modification(csv_path, last_modification_time):
    """
    æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦æœ‰ä¿®æ”¹
    """
    if not os.path.exists(csv_path):
        return False, None
    
    current_mtime = os.path.getmtime(csv_path)
    current_mtime_str = datetime.fromtimestamp(current_mtime).isoformat()
    
    if last_modification_time is None:
        return True, current_mtime_str  # é¦–æ¬¡å¤„ç†
    
    return current_mtime_str != last_modification_time, current_mtime_str

def detect_encoding(file_path):
    """
    æ£€æµ‹æ–‡ä»¶ç¼–ç 
    """
    with open(file_path, 'rb') as f:
        raw_data = f.read(10000)  # è¯»å–å‰10000å­—èŠ‚è¿›è¡Œæ£€æµ‹
        result = chardet.detect(raw_data)
        return result['encoding'], result['confidence']

def detect_separator(file_path, encoding):
    """
    æ£€æµ‹CSVæ–‡ä»¶çš„åˆ†éš”ç¬¦
    """
    try:
        with open(file_path, 'r', encoding=encoding) as f:
            first_line = f.readline()
            
        # æ£€æµ‹å¸¸è§åˆ†éš”ç¬¦
        separators = [',', '\t', ';', '|']
        separator_counts = {}
        
        for sep in separators:
            count = first_line.count(sep)
            if count > 0:
                separator_counts[sep] = count
        
        if separator_counts:
            # é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„åˆ†éš”ç¬¦
            best_separator = max(separator_counts, key=separator_counts.get)
            print(f"æ£€æµ‹åˆ°åˆ†éš”ç¬¦: '{best_separator}' (å‡ºç° {separator_counts[best_separator]} æ¬¡)")
            return best_separator
        else:
            print("æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„åˆ†éš”ç¬¦ï¼Œä½¿ç”¨é»˜è®¤é€—å·")
            return ','
            
    except Exception as e:
        print(f"åˆ†éš”ç¬¦æ£€æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é€—å·")
        return ','

def read_csv_with_encoding(file_path):
    """
    ä½¿ç”¨å¤šç§ç¼–ç å°è¯•è¯»å–CSVæ–‡ä»¶ï¼Œå¹¶è‡ªåŠ¨æ£€æµ‹åˆ†éš”ç¬¦
    """
    # é¦–å…ˆæ£€æµ‹æ–‡ä»¶ç¼–ç 
    encoding, confidence = detect_encoding(file_path)
    print(f"æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {encoding}ï¼Œç½®ä¿¡åº¦: {confidence:.2f}")
    
    # æ£€æµ‹åˆ†éš”ç¬¦
    separator = detect_separator(file_path, encoding)
    
    # å°è¯•ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç å’Œåˆ†éš”ç¬¦è¯»å–
    try:
        df_data = pd.read_csv(file_path, encoding=encoding, sep=separator)
        print(f"ä½¿ç”¨ {encoding} ç¼–ç å’Œ '{separator}' åˆ†éš”ç¬¦æˆåŠŸè¯»å–æ–‡ä»¶")
        print(f"è¯»å–åˆ° {df_data.shape[0]} è¡Œ x {df_data.shape[1]} åˆ—")
        return df_data, encoding
    except Exception as e:
        print(f"ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç  {encoding} è¯»å–å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç ...")
        
        # å°è¯•å¸¸è§ç¼–ç å’Œåˆ†éš”ç¬¦ç»„åˆ
        encodings_to_try = ['utf-16', 'utf-8', 'latin1', 'gbk', 'gb2312', 'gb18030']
        separators_to_try = [separator, ',', '\t', ';', '|']
        
        for enc in encodings_to_try:
            for sep in separators_to_try:
                try:
                    df_data = pd.read_csv(file_path, encoding=enc, sep=sep)
                    if df_data.shape[1] > 1:  # ç¡®ä¿è¯»å–åˆ°å¤šåˆ—
                        print(f"ä½¿ç”¨ {enc} ç¼–ç å’Œ '{sep}' åˆ†éš”ç¬¦æˆåŠŸè¯»å–æ–‡ä»¶")
                        print(f"è¯»å–åˆ° {df_data.shape[0]} è¡Œ x {df_data.shape[1]} åˆ—")
                        return df_data, enc
                except:
                    continue
        
        raise Exception("å°è¯•äº†å¤šç§ç¼–ç å’Œåˆ†éš”ç¬¦ç»„åˆä½†éƒ½å¤±è´¥äº†")

def analyze_data_structure(df):
    """
    åˆ†ææ•°æ®ç»“æ„ï¼Œæ‰“å°åŸºæœ¬ä¿¡æ¯
    """
    print(f"ğŸ“Š æ•°æ®ç»´åº¦: {df.shape[0]} è¡Œ x {df.shape[1]} åˆ—")
    
    # åªæ˜¾ç¤ºæœ‰å¤§é‡ç©ºå€¼çš„å­—æ®µï¼ˆç©ºå€¼æ¯”ä¾‹>30%ï¼‰
    high_null_cols = []
    for col in df.columns:
        null_count = df[col].isnull().sum()
        null_percentage = (null_count / len(df)) * 100
        if null_percentage > 30:
            high_null_cols.append(f"{col}: {null_count} ({null_percentage:.0f}%)")
    
    if high_null_cols:
        print(f"\nå„åˆ—ç©ºå€¼æ•°é‡:")
        for col_info in high_null_cols:
            print(col_info)

def clean_and_convert_data(df):
    """
    æ¸…ç†å’Œè½¬æ¢æ•°æ®ç±»å‹
    """
    print("\n" + "="*60)
    print(" å¼€å§‹æ•°æ®æ¸…æ´—å’Œç±»å‹è½¬æ¢ ")
    print("="*60)
    
    df_cleaned = df.copy()
    
    # 0. é‡å‘½ååˆ—åï¼ˆç®€åŒ–å¤æ‚çš„åˆ—åï¼‰
    column_rename_mapping = {
        'DATE(DATETRUNC(\'day\', [Order Create Time]))': 'Order_Create_Time',
        'DATE(DATETRUNC(\'day\', [Intention Payment Time]))': 'Intention_Payment_Time',
        'DATE(DATETRUNC(\'day\', [Deposit Payment Time]))': 'Deposit_Payment_Time',
        'DATE(DATETRUNC(\'day\', [intention_refund_time]))': 'intention_refund_time',
        'DATE(DATETRUNC(\'day\', [deposit_refund_time]))': 'deposit_refund_time',
        'DATE(DATETRUNC(\'day\', [Lock Time]))': 'Lock_Time',
        'DATE(DATETRUNC(\'day\', [first_touch_time]))': 'first_touch_time',
        'DATE([first_assign_time])': 'first_assign_time',
        'DATE(DATETRUNC(\'day\', DATE([Invoice Upload Time])))': 'Invoice_Upload_Time',
        'DATE(DATETRUNC(\'day\', DATE([store_create_date])))': 'store_create_date',
        'NOT ISNULL([Intention Payment Time])': 'Has_Intention_Payment'
    }
    
    # æ‰§è¡Œé‡å‘½å
    df_cleaned.rename(columns=column_rename_mapping, inplace=True)
    print("âœ… å·²é‡å‘½åä»¥ä¸‹åˆ—å:")
    for old_name, new_name in column_rename_mapping.items():
        if old_name in df.columns:
            print(f"   {old_name} -> {new_name}")
    
    # 1. å¤„ç†æ—¥æœŸåˆ—ï¼ˆä½¿ç”¨é‡å‘½ååçš„åˆ—åï¼‰
    date_columns = [
        'Order_Create_Time',
        'Intention_Payment_Time',
        'Deposit_Payment_Time',
        'intention_refund_time',
        'deposit_refund_time',
        'Lock_Time', 
        'first_touch_time',
        'first_assign_time',
        'Invoice_Upload_Time',
        'store_create_date'
    ]
    
    for col in date_columns:
        if col in df_cleaned.columns:
            try:
                # å°è¯•è½¬æ¢ä¸ºæ—¥æœŸç±»å‹
                df_cleaned[col] = pd.to_datetime(df_cleaned[col], errors='coerce')
                print(f"âœ… æˆåŠŸå°† {col} è½¬æ¢ä¸ºæ—¥æœŸç±»å‹")
            except Exception as e:
                print(f"âŒ è½¬æ¢ {col} æ—¶å‡ºé”™: {e}")

    # ä¿ç•™åŸå§‹å››ä¸ªæ—¥æœŸåˆ—ï¼Œä¸è¿›è¡Œåˆå¹¶æˆ–åˆ é™¤ï¼š
    # Intention_Payment_Timeã€Deposit_Payment_Timeã€intention_refund_timeã€deposit_refund_time
    # ä»¥ä¸Šåˆ—å·²åœ¨æ—¥æœŸè½¬æ¢é˜¶æ®µè¿›è¡Œç±»å‹è½¬æ¢ï¼Œæ— éœ€è¿›ä¸€æ­¥å¤„ç†
    
    # 2. å¤„ç†æ•°å€¼åˆ—ï¼ˆOrder Numberæ˜¯å­—ç¬¦ä¸²IDï¼Œä¸åº”è½¬æ¢ä¸ºæ•°å€¼ï¼‰
    numeric_columns = ['buyer_age', 'Order Number ä¸åŒè®¡æ•°']
    for col in numeric_columns:
        if col in df_cleaned.columns:
            try:
                df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')
                print(f"âœ… æˆåŠŸå°† {col} è½¬æ¢ä¸ºæ•°å€¼ç±»å‹")
            except Exception as e:
                print(f"âŒ è½¬æ¢ {col} æ—¶å‡ºé”™: {e}")
    
    # 2.1 ç‰¹æ®Šå¤„ç†å¼€ç¥¨ä»·æ ¼å­—æ®µï¼ˆæ¸…ç†é€—å·åˆ†éš”ç¬¦ï¼‰
    if 'å¼€ç¥¨ä»·æ ¼' in df_cleaned.columns:
        try:
            # æ¸…ç†é€—å·åˆ†éš”ç¬¦å¹¶è½¬æ¢ä¸ºæ•°å€¼
            df_cleaned['å¼€ç¥¨ä»·æ ¼'] = df_cleaned['å¼€ç¥¨ä»·æ ¼'].astype(str).str.replace(',', '').replace('nan', np.nan)
            df_cleaned['å¼€ç¥¨ä»·æ ¼'] = pd.to_numeric(df_cleaned['å¼€ç¥¨ä»·æ ¼'], errors='coerce')
            print(f"âœ… æˆåŠŸå°† å¼€ç¥¨ä»·æ ¼ è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼ˆå·²æ¸…ç†é€—å·åˆ†éš”ç¬¦ï¼‰")
        except Exception as e:
            print(f"âŒ è½¬æ¢ å¼€ç¥¨ä»·æ ¼ æ—¶å‡ºé”™: {e}")
    
    # 3. å¤„ç†åˆ†ç±»å˜é‡ï¼ˆä½¿ç”¨é‡å‘½ååçš„åˆ—åï¼‰
    category_columns = [
        'è½¦å‹åˆ†ç»„', 'Order Number', 'order_gender', 'first_main_channel_group',
        'Parent Region Name', 'License Province', 'license_city_level', 
        'License City', 'Has_Intention_Payment'
    ]
    
    for col in category_columns:
        if col in df_cleaned.columns:
            try:
                # æ£€æŸ¥å”¯ä¸€å€¼æ¯”ä¾‹ï¼Œå¦‚æœå°äº50%åˆ™è½¬æ¢ä¸ºcategory
                unique_ratio = df_cleaned[col].nunique() / len(df_cleaned)
                if unique_ratio < 0.5:
                    df_cleaned[col] = df_cleaned[col].astype('category')
                    print(f"âœ… å·²å°† {col} è½¬æ¢ä¸ºcategoryç±»å‹ (å”¯ä¸€å€¼æ¯”ä¾‹: {unique_ratio:.2%})")
                else:
                    print(f"âš ï¸  {col} å”¯ä¸€å€¼æ¯”ä¾‹è¿‡é«˜ ({unique_ratio:.2%})ï¼Œä¿æŒä¸ºobjectç±»å‹")
            except Exception as e:
                print(f"âŒ è½¬æ¢ {col} æ—¶å‡ºé”™: {e}")
    
    # 4. å¼ºåˆ¶æ–‡æœ¬ç±»å‹çš„åˆ—ï¼ˆé¿å…ä¿å­˜Parquetæ—¶è¢«é”™è¯¯æ¨æ–­ä¸ºæ•°å€¼ï¼‰
    text_like_patterns = ['Phone', 'ç”µè¯', 'æ‰‹æœºå·', 'æ‰‹æœº', 'ç”µè¯å·']
    explicit_text_columns = ['Store Agent Phone', 'Order Number']
    for col in df_cleaned.columns:
        if col in explicit_text_columns or any(pat in str(col) for pat in text_like_patterns):
            try:
                df_cleaned[col] = df_cleaned[col].astype('string')
                print(f"âœ… å·²å°† {col} è®¾ç½®ä¸ºstringç±»å‹")
            except Exception as e:
                print(f"âŒ è®¾ç½® {col} ä¸ºstringç±»å‹æ—¶å‡ºé”™: {e}")

    return df_cleaned

def optimize_data_types(df):
    """
    ä¼˜åŒ–æ•°æ®ç±»å‹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
    """
    df_optimized = df.copy()
    optimized_count = 0
    
    # å¯¹äºæ•´æ•°åˆ—ï¼Œå°è¯•ä½¿ç”¨æ›´å°çš„æ•°æ®ç±»å‹ï¼ˆOrder Numberä¿æŒä¸ºå­—ç¬¦ä¸²ï¼‰
    int_columns = ['buyer_age', 'Order Number ä¸åŒè®¡æ•°']
    for col in int_columns:
        if col in df_optimized.columns and df_optimized[col].dtype in ['float64', 'int64']:
            # æ£€æŸ¥æ˜¯å¦æœ‰éç©ºå€¼
            if df_optimized[col].notna().any():
                non_null_data = df_optimized[col].dropna()
                if len(non_null_data) == 0:
                    continue
                    
                min_val = non_null_data.min()
                max_val = non_null_data.max()
                
                # æ ¹æ®æ•°æ®èŒƒå›´é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹ï¼ˆä½¿ç”¨å¯ç©ºæ•´æ•°ç±»å‹ï¼‰
                if min_val >= 0 and max_val <= 255:
                    df_optimized[col] = df_optimized[col].astype('UInt8')
                elif min_val >= -128 and max_val <= 127:
                    df_optimized[col] = df_optimized[col].astype('Int8')
                elif min_val >= 0 and max_val <= 65535:
                    df_optimized[col] = df_optimized[col].astype('UInt16')
                elif min_val >= -32768 and max_val <= 32767:
                    df_optimized[col] = df_optimized[col].astype('Int16')
                elif min_val >= 0 and max_val <= 4294967295:
                    df_optimized[col] = df_optimized[col].astype('UInt32')
                elif min_val >= -2147483648 and max_val <= 2147483647:
                    df_optimized[col] = df_optimized[col].astype('Int32')
                else:
                    df_optimized[col] = df_optimized[col].astype('Int64')
                
                optimized_count += 1
    
    if optimized_count > 0:
        print(f"ğŸ”§ å·²ä¼˜åŒ– {optimized_count} ä¸ªå­—æ®µçš„æ•°æ®ç±»å‹")
    
    return df_optimized

def process_intention_order_analysis_to_parquet():
    """
    å¤„ç†æ„å‘è®¢å•åˆ†ææ•°æ®å¹¶è½¬æ¢ä¸ºParquetæ ¼å¼ï¼ˆçœŸæ­£çš„å¢é‡æ›´æ–°ï¼‰
    """
    # å®šä¹‰æ–‡ä»¶è·¯å¾„
    input_file_path = "/Users/zihao_/Documents/coding/dataset/original/Intention_Order_Analysis_(Series_6)_data.csv"
    output_dir = "/Users/zihao_/Documents/coding/dataset/formatted/"
    output_file_path = os.path.join(output_dir, "intention_order_analysis.parquet")
    metadata_file_path = os.path.join(output_dir, "processing_metadata.json")
    
    try:
        print("ğŸš€ å¼€å§‹å¤„ç†æ„å‘è®¢å•åˆ†ææ•°æ®...")
        
        # 0. åŠ è½½å¤„ç†å…ƒæ•°æ®
        metadata = load_processing_metadata(metadata_file_path)
        
        # 1. æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦æœ‰ä¿®æ”¹
        has_changes, current_mtime = check_csv_modification(
            input_file_path, 
            metadata.get('last_csv_modification_time')
        )
        
        if not has_changes and metadata.get('incremental_mode', False):
            if os.path.exists(output_file_path):
                df_existing = pd.read_parquet(output_file_path)
                print(f"âœ… æºæ–‡ä»¶æœªå˜åŒ–ï¼Œè¿”å›ç°æœ‰æ•°æ®: {df_existing.shape[0]} è¡Œ")
                return df_existing, output_file_path
        
        # 2. æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†å²Parquetæ–‡ä»¶
        df_existing = None
        processing_mode = "full"  # full: å…¨é‡å¤„ç†, incremental: å¢é‡å¤„ç†
        
        if os.path.exists(output_file_path):
            try:
                df_existing = pd.read_parquet(output_file_path)
                print(f"ğŸ“š å†å²æ•°æ®: {df_existing.shape[0]} è¡Œ")
                processing_mode = "incremental"
            except Exception as e:
                print(f"âŒ è¯»å–å†å²æ•°æ®å¤±è´¥: {e}")
                df_existing = None
                processing_mode = "full"
        
        # 2. è¯»å–æ–°çš„CSVæ–‡ä»¶
        df_raw, encoding = read_csv_with_encoding(input_file_path)
        print(f"ğŸ“– æ–°æ•°æ®: {df_raw.shape[0]} è¡Œ")
        
        # 3. åˆ†ææ•°æ®ç»“æ„ï¼ˆä»…å¯¹æ–°æ•°æ®ï¼‰
        analyze_data_structure(df_raw)
        
        # 4. æ¸…ç†å’Œè½¬æ¢æ•°æ®ç±»å‹
        df_cleaned = clean_and_convert_data(df_raw)
        
        # 5. ä¼˜åŒ–æ•°æ®ç±»å‹
        df_new = optimize_data_types(df_cleaned)
        
        # 6. æ™ºèƒ½æ•°æ®å¤„ç†å’Œåˆå¹¶
        print("\n" + "="*60)
        print(" æ™ºèƒ½æ•°æ®å¤„ç†å’Œåˆå¹¶ ")
        print("="*60)
        
        if processing_mode == "incremental" and df_existing is not None:
            print(f"ğŸ“Š å¢é‡æ¨¡å¼: æ™ºèƒ½åˆå¹¶å†å²æ•°æ®å’Œæ–°æ•°æ®...")
            print(f"å†å²æ•°æ®: {df_existing.shape[0]} è¡Œ")
            print(f"æ–°æ•°æ®: {df_new.shape[0]} è¡Œ")
            
            # ç¡®ä¿ä¸¤ä¸ªæ•°æ®æ¡†æœ‰ç›¸åŒçš„åˆ—ç»“æ„
            if set(df_existing.columns) != set(df_new.columns):
                print("âš ï¸  è­¦å‘Š: å†å²æ•°æ®å’Œæ–°æ•°æ®çš„åˆ—ç»“æ„ä¸å®Œå…¨ä¸€è‡´")
                print(f"å†å²æ•°æ®åˆ—æ•°: {len(df_existing.columns)}")
                print(f"æ–°æ•°æ®åˆ—æ•°: {len(df_new.columns)}")
                
                # è·å–å…±åŒåˆ—å’Œå·®å¼‚åˆ—
                common_cols = list(set(df_existing.columns) & set(df_new.columns))
                existing_only = set(df_existing.columns) - set(df_new.columns)
                new_only = set(df_new.columns) - set(df_existing.columns)
                
                print(f"å…±åŒåˆ—: {len(common_cols)} ä¸ª")
                if existing_only:
                    print(f"å†å²æ•°æ®ç‹¬æœ‰åˆ—: {list(existing_only)}")
                if new_only:
                    print(f"æ–°æ•°æ®ç‹¬æœ‰åˆ—: {list(new_only)}")
                
                # æ™ºèƒ½åˆ—å¯¹é½ï¼šä¿ç•™æ‰€æœ‰åˆ—ï¼Œç¼ºå¤±çš„ç”¨NaNå¡«å……
                all_cols = list(set(df_existing.columns) | set(df_new.columns))
                
                # ä¸ºå†å²æ•°æ®æ·»åŠ ç¼ºå¤±åˆ—
                for col in new_only:
                    df_existing[col] = pd.NA
                
                # ä¸ºæ–°æ•°æ®æ·»åŠ ç¼ºå¤±åˆ—
                for col in existing_only:
                    df_new[col] = pd.NA
                
                # é‡æ–°æ’åºåˆ—ä»¥ä¿æŒä¸€è‡´æ€§
                df_existing = df_existing[all_cols]
                df_new = df_new[all_cols]
                
                print(f"âœ… åˆ—ç»“æ„å·²å¯¹é½: {len(all_cols)} åˆ—")
            
            # æ™ºèƒ½åˆå¹¶ï¼šå¤„ç†æ–°å¢å’Œæ›´æ–°çš„è®¢å•è®°å½•
            if 'Order Number' in df_new.columns and 'Order Number' in df_existing.columns:
                print(f"ğŸ” åˆ†æè®¢å•æ•°æ®å˜åŒ–...")
                existing_orders = set(df_existing['Order Number'].dropna())
                new_orders = set(df_new['Order Number'].dropna())
                
                truly_new_orders = new_orders - existing_orders
                updated_orders = new_orders & existing_orders
                removed_orders = existing_orders - new_orders
                
                print(f"ç°æœ‰è®¢å•æ•°: {len(existing_orders)}")
                print(f"æ–°æ–‡ä»¶è®¢å•æ•°: {len(new_orders)}")
                print(f"çœŸæ­£æ–°å¢è®¢å•: {len(truly_new_orders)}")
                print(f"å¯èƒ½æ›´æ–°çš„è®¢å•: {len(updated_orders)}")
                print(f"å¯èƒ½ç§»é™¤çš„è®¢å•: {len(removed_orders)}")
                
                # ä¿®æ­£çš„å¢é‡æ›´æ–°é€»è¾‘ï¼šåªæœ‰åœ¨æ˜ç¡®æ£€æµ‹åˆ°æ˜¯å…¨é‡å¯¼å‡ºæ—¶æ‰æ›¿æ¢
                # åˆ¤æ–­æ ‡å‡†ï¼šå¦‚æœç§»é™¤çš„è®¢å•æ•°é‡è¶…è¿‡æ€»è®¢å•æ•°çš„50%ï¼Œä¸”æ–°å¢è®¢å•å¾ˆå°‘ï¼Œå¯èƒ½æ˜¯å…¨é‡å¿«ç…§
                total_existing = len(existing_orders)
                removal_ratio = len(removed_orders) / total_existing if total_existing > 0 else 0
                new_ratio = len(truly_new_orders) / len(new_orders) if len(new_orders) > 0 else 0
                
                # æ›´ä¸¥æ ¼çš„åˆ¤æ–­æ¡ä»¶ï¼šåªæœ‰åœ¨ç§»é™¤æ¯”ä¾‹å¾ˆé«˜ä¸”æ–°å¢æ¯”ä¾‹å¾ˆä½æ—¶æ‰è®¤ä¸ºæ˜¯å®Œæ•´å¿«ç…§
                # åŒæ—¶è¦æ±‚æ–°æ–‡ä»¶çš„è®¢å•æ•°é‡æ˜¾è‘—å°äºå†å²æ•°æ®ï¼Œè¿™é€šå¸¸è¡¨æ˜æ˜¯æ•°æ®ç­›é€‰æˆ–æ—¶é—´èŒƒå›´å˜åŒ–
                size_reduction_ratio = len(new_orders) / total_existing if total_existing > 0 else 1
                
                # æ›´ä¿å®ˆçš„å®Œæ•´å¿«ç…§åˆ¤æ–­ï¼šåªæœ‰åœ¨æç«¯æƒ…å†µä¸‹æ‰è®¤ä¸ºæ˜¯å®Œæ•´å¿«ç…§
                # 1. ç§»é™¤æ¯”ä¾‹è¶…è¿‡98%ï¼ˆéå¸¸é«˜çš„ç§»é™¤æ¯”ä¾‹ï¼‰
                # 2. æ–°å¢è®¢å•æ¯”ä¾‹ä½äº1%ï¼ˆå‡ ä¹æ²¡æœ‰æ–°è®¢å•ï¼‰
                # 3. æ–°æ–‡ä»¶å¤§å°ç›¸æ¯”å†å²æ•°æ®æ˜¾è‘—å‡å°‘ï¼ˆå°äº20%ï¼‰
                # 4. æ–°æ–‡ä»¶è®¢å•æ•°é‡è¶…è¿‡5000ï¼ˆç¡®ä¿æ˜¯å¤§è§„æ¨¡æ•°æ®ï¼‰
                # 5. ç§»é™¤çš„è®¢å•æ•°é‡è¶…è¿‡æ–°å¢è®¢å•æ•°é‡çš„100å€ï¼ˆé¿å…æ­£å¸¸çš„æ•°æ®æ›´æ–°è¢«è¯¯åˆ¤ï¼‰
                is_full_snapshot = (
                    removal_ratio > 0.98 and 
                    new_ratio < 0.01 and 
                    size_reduction_ratio < 0.2 and 
                    len(new_orders) > 5000 and
                    len(removed_orders) > len(truly_new_orders) * 100
                )
                
                if is_full_snapshot:
                    print(f"ğŸ”„ æ£€æµ‹åˆ°å®Œæ•´å¿«ç…§ï¼Œé‡‡ç”¨å®Œæ•´æ›¿æ¢ç­–ç•¥")
                    
                    # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ•°æ®å‡å°‘è¿‡å¤šï¼Œè¦æ±‚ç”¨æˆ·ç¡®è®¤
                    data_loss_ratio = 1 - size_reduction_ratio
                    if data_loss_ratio > 0.5:  # å¦‚æœæ•°æ®å‡å°‘è¶…è¿‡50%
                        print(f"âš ï¸  è­¦å‘Š: æ–°æ•°æ®ç›¸æ¯”å†å²æ•°æ®å‡å°‘äº† {data_loss_ratio:.1%}")
                    
                    df_final = df_new.copy()
                    print(f"âœ… ä½¿ç”¨æ–°æ•°æ®å®Œå…¨æ›¿æ¢: {df_final.shape[0]} è¡Œ")
                else:
                     print(f"ğŸ“ˆ é‡‡ç”¨å¢é‡æ›´æ–°ç­–ç•¥ (ç§»é™¤:{removal_ratio:.1%}, æ–°å¢:{new_ratio:.1%})")
                     
                     # å¼€å§‹æ„å»ºæœ€ç»ˆæ•°æ®é›†
                     df_final = df_existing.copy()
                     
                     if len(truly_new_orders) > 0:
                         # æ·»åŠ çœŸæ­£çš„æ–°è®¢å•
                         df_new_records = df_new[df_new['Order Number'].isin(truly_new_orders)]
                         print(f"ğŸ“ˆ æ·»åŠ  {len(df_new_records)} æ¡æ–°è®°å½•")
                         
                         # åˆå¹¶æ•°æ®
                         df_final = pd.concat([df_final, df_new_records], ignore_index=True)
                         print(f"åˆå¹¶åæ€»æ•°æ®: {df_final.shape[0]} è¡Œ")
                     else:
                         print("âœ… æœªå‘ç°æ–°è®¢å•")
                
                     # å¤„ç†å¯èƒ½çš„æ›´æ–°è®°å½•ï¼šåŸºäºæ—¶é—´æˆ³å’Œå…³é”®å­—æ®µçš„æ™ºèƒ½æ›´æ–°ç­–ç•¥ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ï¼‰
                     if len(updated_orders) > 0:
                         print(f"ğŸ”„ æ£€æŸ¥ {len(updated_orders)} ä¸ªè®¢å•çš„æ›´æ–°...")
                         
                         # å®šä¹‰å…³é”®æ—¶é—´å­—æ®µ
                         time_columns = ['Order_Create_Time', 'Intention_Payment_Time', 'intention_refund_time', 'Lock_Time', 'Invoice_Upload_Time','store_create_date']
                         available_time_cols = [col for col in time_columns if col in df_new.columns and col in df_existing.columns]
                         
                         # å®šä¹‰å…³é”®ä¸šåŠ¡å­—æ®µï¼ˆéæ—¶é—´å­—æ®µï¼‰
                         business_columns = ['Product Name', 'è½¦å‹åˆ†ç»„', 'å¼€ç¥¨ä»·æ ¼', 'buyer_age', 'order_gender', 'License Province', 'License City']
                         available_business_cols = [col for col in business_columns if col in df_new.columns and col in df_existing.columns]
                         
                         # è·å–éœ€è¦æ›´æ–°çš„è®¢å•æ•°æ®ï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
                         df_updated_records = df_new[df_new['Order Number'].isin(updated_orders)].copy()
                         df_existing_updated = df_existing[df_existing['Order Number'].isin(updated_orders)].copy()
                         
                         # è®¾ç½®ç´¢å¼•ä»¥ä¾¿å¿«é€Ÿåˆå¹¶æ¯”è¾ƒ
                         df_updated_records = df_updated_records.set_index('Order Number')
                         df_existing_updated = df_existing_updated.set_index('Order Number')
                         
                         # å‘é‡åŒ–æ¯”è¾ƒå­—æ®µ
                         orders_to_update = set()
                         update_stats = {}
                         
                         # æ£€æŸ¥æ—¶é—´å­—æ®µæ›´æ–°
                         if available_time_cols:
                             for time_col in available_time_cols:
                                 # å‘é‡åŒ–æ¯”è¾ƒï¼šæ‰¾å‡ºæœ‰æ›´æ–°çš„è®¢å•
                                 # ç¡®ä¿ä¸¤ä¸ªSeriesæœ‰ç›¸åŒçš„ç´¢å¼•
                                 common_orders = df_updated_records.index.intersection(df_existing_updated.index)
                                 
                                 if len(common_orders) > 0:
                                     new_times = df_updated_records.loc[common_orders, time_col]
                                     existing_times = df_existing_updated.loc[common_orders, time_col]
                                     
                                     # æ‰¾å‡ºæ–°æ•°æ®ä¸ä¸ºç©ºä¸”ä¸ç°æœ‰æ•°æ®ä¸åŒçš„è®¢å•
                                     has_new_data = pd.notna(new_times)
                                     is_different = (pd.isna(existing_times) | (new_times != existing_times))
                                     needs_update = has_new_data & is_different
                                     
                                     updated_orders_for_col = needs_update[needs_update].index.tolist()
                                     
                                     if updated_orders_for_col:
                                         orders_to_update.update(updated_orders_for_col)
                                         update_stats[time_col] = len(updated_orders_for_col)
                         
                         # æ£€æŸ¥ä¸šåŠ¡å­—æ®µæ›´æ–°
                         if available_business_cols:
                             for business_col in available_business_cols:
                                 # å‘é‡åŒ–æ¯”è¾ƒï¼šæ‰¾å‡ºæœ‰æ›´æ–°çš„è®¢å•
                                 common_orders = df_updated_records.index.intersection(df_existing_updated.index)
                                 
                                 if len(common_orders) > 0:
                                     new_values = df_updated_records.loc[common_orders, business_col].astype(str)
                                     existing_values = df_existing_updated.loc[common_orders, business_col].astype(str)
                                     
                                     # æ‰¾å‡ºå€¼ä¸åŒçš„è®¢å•ï¼ˆåŒ…æ‹¬ä»ç©ºå€¼åˆ°æœ‰å€¼çš„æƒ…å†µï¼‰
                                     is_different = (new_values != existing_values)
                                     
                                     # æ’é™¤ä¸¤è¾¹éƒ½æ˜¯NaNçš„æƒ…å†µ
                                     both_nan = (new_values == 'nan') & (existing_values == 'nan')
                                     needs_update = is_different & ~both_nan
                                     
                                     updated_orders_for_col = needs_update[needs_update].index.tolist()
                                     
                                     if updated_orders_for_col:
                                         orders_to_update.update(updated_orders_for_col)
                                         update_stats[business_col] = len(updated_orders_for_col)
                         
                         if orders_to_update:
                             # æ±‡æ€»æ˜¾ç¤ºæ›´æ–°ç»Ÿè®¡
                             update_summary = ", ".join([f"{col}:{count}ä¸ª" for col, count in update_stats.items()])
                             print(f"ğŸ“ˆ å‘ç° {len(orders_to_update)} ä¸ªè®¢å•éœ€è¦æ›´æ–° ({update_summary})")
                             
                             # ç§»é™¤æ—§è®°å½•
                             df_final = df_final[~df_final['Order Number'].isin(orders_to_update)]
                             
                             # æ·»åŠ æ›´æ–°åçš„è®°å½•
                             df_updated_final = df_new[df_new['Order Number'].isin(orders_to_update)]
                             df_final = pd.concat([df_final, df_updated_final], ignore_index=True)
                             
                             print(f"âœ… å·²æ›´æ–° {len(orders_to_update)} ä¸ªè®¢å•çš„è®°å½•")
                         else:
                             print(f"âœ… é‡å¤è®¢å•æ— å­—æ®µæ›´æ–°ï¼Œä¿æŒç°æœ‰æ•°æ®")
                         
                         print(f"æœ€ç»ˆæ•°æ®: {df_final.shape[0]} è¡Œ")
            else:
                print("âš ï¸  æœªæ‰¾åˆ° 'Order Number' åˆ—ï¼Œæ‰§è¡Œç®€å•åˆå¹¶")
                df_final = pd.concat([df_existing, df_new], ignore_index=True)
                
        else:
            print(f"ğŸ“ å…¨é‡æ¨¡å¼: ç›´æ¥ä½¿ç”¨æ–°æ•°æ®")
            df_final = df_new
        
        # 7. æ•°æ®è´¨é‡æ£€æŸ¥å‰çš„ç±»å‹ç»Ÿä¸€ï¼ˆé¿å…Parquetä¿å­˜æ—¶çš„æ··åˆç±»å‹é—®é¢˜ï¼‰
        safe_string_cols = [
            'Order Number', 'Store Agent Phone', 'Buyer Cell Phone',
            'Store Agent Id', 'Buyer Identity No', 'Store Code'
        ]
        for col in safe_string_cols:
            if col in df_final.columns:
                try:
                    df_final[col] = df_final[col].astype('string')
                    # æ‰“å°ä¸€æ¬¡å³å¯ï¼Œé¿å…åˆ·å±
                    print(f"ğŸ”’ å·²ç»Ÿä¸€ {col} ä¸ºstringç±»å‹ä»¥ç¡®ä¿å†™å…¥å®‰å…¨")
                except Exception as e:
                    print(f"âš ï¸  å°† {col} ç»Ÿä¸€ä¸ºstringç±»å‹å¤±è´¥: {e}")

        # 7. æ•°æ®è´¨é‡æ£€æŸ¥
        print("\n" + "="*60)
        print(" æœ€ç»ˆæ•°æ®è´¨é‡æŠ¥å‘Š ")
        print("="*60)
        print(f"æœ€ç»ˆæ•°æ®ç»´åº¦: {df_final.shape}")
        
        if 'Order_Create_Time' in df_final.columns:
            print(f"ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {df_final['Order_Create_Time'].min()} åˆ° {df_final['Order_Create_Time'].max()}")
        
        print(f"\nå„åˆ—æ•°æ®ç±»å‹:")
        print(df_final.dtypes)
        
        print(f"\nå„åˆ—ç©ºå€¼æ•°é‡:")
        null_counts = df_final.isnull().sum()
        for col, count in null_counts.items():
            if count > 0:
                percentage = (count / len(df_final)) * 100
                print(f"{col}: {count} ({percentage:.2f}%)")
        
        # 8. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        
        # 9. ä¿å­˜ä¸ºParquetæ–‡ä»¶
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æœ€ç»ˆæ•°æ®...")
        df_final.to_parquet(output_file_path, index=False, engine='pyarrow', compression='snappy')
        
        # 10. æ›´æ–°å¤„ç†å…ƒæ•°æ®
        print(f"\nğŸ“‹ æ›´æ–°å¤„ç†å…ƒæ•°æ®...")
        current_time = datetime.now().isoformat()
        
        # æ›´æ–°å…ƒæ•°æ®
        metadata.update({
            'last_processed_timestamp': current_time,
            'last_csv_modification_time': current_mtime,
            'last_processing_time': current_time,
            'total_records_processed': len(df_final),
            'data_version': f"{metadata.get('data_version', '1.0.0')}",
            'processing_mode': processing_mode
        })
        
        # æ·»åŠ å¤„ç†å†å²è®°å½•
        processing_record = {
            'timestamp': current_time,
            'mode': processing_mode,
            'input_file_mtime': current_mtime,
            'records_before': len(df_existing) if df_existing is not None else 0,
            'records_after': len(df_final),
            'new_records_added': len(df_final) - (len(df_existing) if df_existing is not None else 0)
        }
        
        if 'processing_history' not in metadata:
            metadata['processing_history'] = []
        metadata['processing_history'].append(processing_record)
        
        # ä¿æŒå†å²è®°å½•ä¸è¶…è¿‡50æ¡
        if len(metadata['processing_history']) > 50:
            metadata['processing_history'] = metadata['processing_history'][-50:]
        
        # ä¿å­˜å…ƒæ•°æ®
        if save_processing_metadata(metadata_file_path, metadata):
            print(f"âœ… å…ƒæ•°æ®å·²æ›´æ–°")
        else:
            print(f"âš ï¸  å…ƒæ•°æ®æ›´æ–°å¤±è´¥")
        
        # 11. è®¡ç®—æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(output_file_path) / (1024 * 1024)  # MB
        
        print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file_path}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        print(f"ğŸ“ˆ æœ€ç»ˆæ•°æ®ç»´åº¦: {df_final.shape[0]} è¡Œ x {df_final.shape[1]} åˆ—")
        print(f"ğŸ”§ å¤„ç†æ¨¡å¼: {processing_mode}")
        
        # 12. æ˜¾ç¤ºæœ€ç»ˆæ•°æ®æ ·æœ¬
        print(f"\næœ€ç»ˆæ•°æ®æ ·æœ¬ï¼ˆå‰5è¡Œï¼‰:")
        print(df_final.head())
        
        return df_final, output_file_path
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise e

if __name__ == "__main__":
    # æ‰§è¡Œæ•°æ®å¤„ç†
    try:
        df, output_path = process_intention_order_analysis_to_parquet()
        print("\nğŸ‰ æ„å‘è®¢å•åˆ†ææ•°æ®å¤„ç†å®Œæˆï¼")
    except Exception as e:
        print(f"\nğŸ’¥ å¤„ç†å¤±è´¥: {e}")
