#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Order å®Œæ•´æ•°æ®å¤„ç†è„šæœ¬

è¯¥è„šæœ¬ç”¨äºå¤„ç† Order_å®Œæ•´æ•°æ®_data.csvã€Order_å®Œæ•´æ•°æ®_data_2024.csvï¼Œä»¥åŠ original ç›®å½•ä¸‹æœ€æ–°çš„å¹´åº¦æ–‡ä»¶ï¼ˆå¦‚ Order_å®Œæ•´æ•°æ®_data_2025*.csvï¼‰
å°†å…¶åˆå¹¶ã€å»é‡å¹¶è½¬æ¢ä¸ºä¼˜åŒ–çš„ Parquet æ ¼å¼

è¾“å…¥æ–‡ä»¶: 
- original/Order_å®Œæ•´æ•°æ®_data.csv
- original/Order_å®Œæ•´æ•°æ®_data_2024.csv
- original/Order_å®Œæ•´æ•°æ®_data_2025*.csvï¼ˆé€‰æœ€æ–°çš„ä¸€ä¸ªï¼‰
è¾“å‡ºæ–‡ä»¶: formatted/order_full_data.parquet
"""

import pandas as pd
import numpy as np
import os
from pathlib import Path
from datetime import datetime

# è®¾ç½®åŸºç¡€è·¯å¾„
BASE_DIR = Path("/Users/zihao_/Documents/coding/dataset")
ORIGINAL_DIR = BASE_DIR / "original"
FORMATTED_DIR = BASE_DIR / "formatted"
OUTPUT_FILE = FORMATTED_DIR / "order_full_data.parquet"

def read_csv_smart(file_path: Path) -> pd.DataFrame:
    """
    æ™ºèƒ½è¯»å– CSV æ–‡ä»¶ï¼Œå°è¯•å¤šç§ç¼–ç å’Œåˆ†éš”ç¬¦
    """
    if not file_path.exists():
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return pd.DataFrame()

    print(f"ğŸ“– æ­£åœ¨è¯»å–: {file_path.name} ...")
    
    # å¸¸è§ç¼–ç å’Œåˆ†éš”ç¬¦ç»„åˆ
    encodings = ["utf-16", "utf-8", "utf-8-sig", "gb18030", "gbk"]
    separators = ["\t", ","]
    
    for enc in encodings:
        for sep in separators:
            try:
                df = pd.read_csv(file_path, encoding=enc, sep=sep)
                
                # ç®€å•éªŒè¯è¯»å–æ˜¯å¦æˆåŠŸï¼šå¦‚æœåˆ—æ•°åªæœ‰1ä¸”åŒ…å«åˆ†éš”ç¬¦ï¼Œè¯´æ˜åˆ†éš”ç¬¦ä¸å¯¹
                if df.shape[1] == 1 and sep in str(df.columns[0]):
                    continue
                
                # å¦‚æœåˆ—æ•°å¤§äº1ï¼Œé€šå¸¸è¯´æ˜è¯»å–æ­£ç¡®
                if df.shape[1] > 1:
                    print(f"âœ… è¯»å–æˆåŠŸ (ç¼–ç : {enc}, åˆ†éš”ç¬¦: '{sep if sep != '\t' else '\\t'}') - å½¢çŠ¶: {df.shape}")
                    return df
            except Exception:
                continue
                
    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå°è¯•é»˜è®¤è¯»å–
    try:
        print("âš ï¸ å°è¯•é»˜è®¤å‚æ•°è¯»å–...")
        return pd.read_csv(file_path)
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        return pd.DataFrame()

def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ¸…ç†åˆ—åï¼šå»é™¤ç©ºç™½å­—ç¬¦ï¼Œç»Ÿä¸€å‘½åé£æ ¼
    """
    # å»é™¤å‰åç©ºæ ¼
    df.columns = df.columns.str.strip()
    
    # é¢„å¤„ç†ï¼šç»Ÿä¸€å°† 'xxx_å¹´/æœˆ/æ—¥' æ ¼å¼è½¬æ¢ä¸º 'xxx å¹´/æœˆ/æ—¥'ï¼Œä»¥åŒ¹é…ä¸‹æ–¹çš„æ˜ å°„è¡¨
    # è¿™æ ·å¯ä»¥å…¼å®¹ä¸‹åˆ’çº¿å’Œç©ºæ ¼ä¸¤ç§åˆ†éš”ç¬¦
    df.columns = df.columns.str.replace('_å¹´/æœˆ/æ—¥', ' å¹´/æœˆ/æ—¥', regex=False)

    # é‡å‘½åæ˜ å°„è¡¨ï¼ˆæ ¹æ®ä¹‹å‰çš„åˆ†ææŠ¥å‘Šï¼‰
    rename_map = {
        'first_touch_time å¹´/æœˆ/æ—¥': 'first_touch_time',
        'delivery_date å¹´/æœˆ/æ—¥': 'delivery_date',
        'deposit_payment_time å¹´/æœˆ/æ—¥': 'deposit_payment_time',
        'deposit_refund_time å¹´/æœˆ/æ—¥': 'deposit_refund_time',
        'first_test_drive_time å¹´/æœˆ/æ—¥': 'first_test_drive_time',
        'intention_payment_time å¹´/æœˆ/æ—¥': 'intention_payment_time',
        'intention_refund_time å¹´/æœˆ/æ—¥': 'intention_refund_time',
        'invoice_upload_time å¹´/æœˆ/æ—¥': 'invoice_upload_time',
        'lock_time å¹´/æœˆ/æ—¥': 'lock_time',
        'order_create_time å¹´/æœˆ/æ—¥': 'order_create_date', # åŒºåˆ† order_create_time
        'store_create_date å¹´/æœˆ/æ—¥': 'store_create_date',
        'approve_refund_time å¹´/æœˆ/æ—¥': 'approve_refund_time',
        'apply_refund_time å¹´/æœˆ/æ—¥': 'apply_refund_time',
        'first_assign_time å¹´/æœˆ/æ—¥': 'first_assign_time',
        'lead_assign_time_max å¹´/æœˆ/æ—¥': 'lead_assign_time_max',
        'Td CountD': 'td_countd',
        'Drive Series Cn': 'drive_series_cn',
        'Main Lead Id': 'main_lead_id',
        'Parent Region Name': 'parent_region_name',
        'Parent_Region_Name': 'parent_region_name',
    }
    
    # åº”ç”¨é‡å‘½å
    df = df.rename(columns=rename_map)
    
    # å°†å‰©ä½™åˆ—åè½¬æ¢ä¸ºä¸‹åˆ’çº¿é£æ ¼ï¼ˆå¦‚æœå·²ç»æ˜¯è‹±æ–‡ï¼‰
    # è¿™é‡Œç®€å•å¤„ç†ï¼Œåªæ›¿æ¢ç©ºæ ¼
    df.columns = df.columns.str.replace(' ', '_')
    
    return df

def convert_types(df: pd.DataFrame) -> pd.DataFrame:
    """
    è½¬æ¢æ•°æ®ç±»å‹
    """
    print("ğŸ”„ å¼€å§‹ç±»å‹è½¬æ¢...")
    
    # 1. æ—¥æœŸåˆ—è½¬æ¢
    date_cols = [
        'first_touch_time', 'delivery_date', 'deposit_payment_time', 
        'deposit_refund_time', 'first_test_drive_time', 'intention_payment_time', 
        'intention_refund_time', 'invoice_upload_time', 'lock_time', 
        'order_create_date', 'store_create_date', 'order_create_time',
        'approve_refund_time', 'apply_refund_time'
    ]
    
    for col in date_cols:
        if col in df.columns:
            # å¤„ç†ä¸­æ–‡æ—¥æœŸæ ¼å¼ (YYYYå¹´MMæœˆDDæ—¥)
            # å…ˆå°† series è½¬ä¸º string
            s = df[col].astype(str)
            # æ›¿æ¢å¹´æœˆæ—¥
            s = s.str.replace('å¹´', '-', regex=False).str.replace('æœˆ', '-', regex=False).str.replace('æ—¥', '', regex=False)
            # å¤„ç†å¯èƒ½çš„ 'nan' å­—ç¬¦ä¸²
            s = s.replace({'nan': None, 'None': None, '': None})
            
            df[col] = pd.to_datetime(s, errors='coerce')
            print(f"   - æ—¥æœŸåˆ—è½¬æ¢: {col}")

    # 2. æ•°å€¼åˆ—è½¬æ¢
    numeric_cols = ['age', 'invoice_amount', 'td_countd']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            print(f"   - æ•°å€¼åˆ—è½¬æ¢: {col}")

    # 3. åˆ†ç±»åˆ—è½¬æ¢ (ä¼˜åŒ–å­˜å‚¨)
    cat_cols = [
        'product_name', 'final_payment_way', 'finance_product', 
        'first_middle_channel_name', 'gender', 'is_hold', 'is_staff',
        'license_city', 'license_city_level', 'license_province',
        'order_type', 'series', 'store_city', 'belong_intent_series',
        'drive_series_cn', 'parent_region_name'
    ]
    
    for col in cat_cols:
        if col in df.columns:
            # å¦‚æœå”¯ä¸€å€¼æ•°é‡è¾ƒå°‘ï¼Œè½¬ä¸º category
            if df[col].nunique() < df.shape[0] * 0.5:
                df[col] = df[col].astype('category')
                print(f"   - åˆ†ç±»åˆ—è½¬æ¢: {col}")
            else:
                df[col] = df[col].astype('string')

    # order_number åº”è¯¥æ˜¯å­—ç¬¦ä¸²
    if 'order_number' in df.columns:
        df['order_number'] = df['order_number'].astype('string')

    return df

def main():
    # 1. æŒ‰è¦æ±‚æ”¶é›†è¾“å…¥æ–‡ä»¶ï¼šåŸºç¡€æ–‡ä»¶ + 2024å¹´åº¦ + æœ€æ–°çš„å½“å‰å¹´åº¦æ–‡ä»¶
    csv_files = []
    
    base_files = [
        ORIGINAL_DIR / "Order_å®Œæ•´æ•°æ®_data.csv",
        ORIGINAL_DIR / "Order_å®Œæ•´æ•°æ®_data_2024.csv",
    ]
    for bf in base_files:
        if bf.exists():
            csv_files.append(bf)
    
    current_year = datetime.now().strftime("%Y")
    year_pattern = f"Order_å®Œæ•´æ•°æ®_data_{current_year}*.csv"
    year_files = list(ORIGINAL_DIR.glob(year_pattern))
    if year_files:
        latest_year_file = max(year_files, key=lambda p: p.stat().st_mtime)
        csv_files.append(latest_year_file)
    
    if not csv_files:
        print(f"âŒ æœªåœ¨ {ORIGINAL_DIR} æ‰¾åˆ°æ‰€éœ€çš„è¾“å…¥æ–‡ä»¶ï¼ˆåŸºç¡€ã€2024æˆ–å½“å¹´æœ€æ–°ï¼‰")
        return

    print(f"ğŸ” æ‰¾åˆ° {len(csv_files)} ä¸ªæ•°æ®æ–‡ä»¶ï¼Œå°†æŒ‰ä»¥ä¸‹é¡ºåºå¤„ç†:")
    for f in csv_files:
        print(f"   - {f.name}")
    
    # 2. è¯»å–å¹¶åˆå¹¶æ‰€æœ‰æ–°æ•°æ®
    dfs = []
    for file_path in csv_files:
        df = read_csv_smart(file_path)
        if not df.empty:
            # æ¸…ç†åˆ—åå’Œè½¬æ¢ç±»å‹
            # æ³¨æ„ï¼šå¿…é¡»åœ¨åˆå¹¶å‰æ¸…ç†åˆ—åï¼Œä»¥ç¡®ä¿åˆ—åä¸€è‡´
            df = clean_column_names(df)
            df = convert_types(df)
            dfs.append(df)
        
    if not dfs:
        print("âŒ æ²¡æœ‰æˆåŠŸè¯»å–åˆ°ä»»ä½•æ•°æ®ï¼Œé€€å‡ºã€‚")
        return
        
    df_new = pd.concat(dfs, ignore_index=True)
    print(f"âœ… æ‰€æœ‰æ–°æ•°æ®åˆå¹¶å®Œæˆ: {df_new.shape[0]} è¡Œ")

    # 4. å¢é‡æ›´æ–°é€»è¾‘
    if OUTPUT_FILE.exists():
        print(f"ğŸ“š å‘ç°ç°æœ‰ Parquet æ–‡ä»¶: {OUTPUT_FILE}")
        try:
            df_existing = pd.read_parquet(OUTPUT_FILE)
            print(f"   ç°æœ‰æ•°æ®: {df_existing.shape[0]} è¡Œ")
            legacy_map = {
                'approve_refund_time_å¹´/æœˆ/æ—¥': 'approve_refund_time',
                'apply_refund_time_å¹´/æœˆ/æ—¥': 'apply_refund_time',
                'approve_refund_time å¹´/æœˆ/æ—¥': 'approve_refund_time',
                'apply_refund_time å¹´/æœˆ/æ—¥': 'apply_refund_time',
                'first_assign_time_å¹´/æœˆ/æ—¥': 'first_assign_time',
                'lead_assign_time_max_å¹´/æœˆ/æ—¥': 'lead_assign_time_max',
                'first_assign_time å¹´/æœˆ/æ—¥': 'first_assign_time',
                'lead_assign_time_max å¹´/æœˆ/æ—¥': 'lead_assign_time_max',
                'Parent Region Name': 'parent_region_name',
                'Parent_Region_Name': 'parent_region_name'
            }
            for old_col, new_col in legacy_map.items():
                if old_col in df_existing.columns:
                    if df_existing[old_col].dtype == 'object':
                        try:
                            s = df_existing[old_col].astype(str)
                            s = s.str.replace('å¹´', '-', regex=False).str.replace('æœˆ', '-', regex=False).str.replace('æ—¥', '', regex=False)
                            s = s.replace({'nan': None, 'None': None, '': None})
                            df_existing[old_col] = pd.to_datetime(s, errors='coerce')
                        except Exception as e:
                            print(f"      âš ï¸ è½¬æ¢å¤±è´¥: {e}")
                    if new_col in df_existing.columns:
                        df_existing[new_col] = df_existing[new_col].combine_first(df_existing[old_col])
                        df_existing = df_existing.drop(columns=[old_col])
                    else:
                        df_existing = df_existing.rename(columns={old_col: new_col})
            common_cols = list(set(df_existing.columns) & set(df_new.columns))
            new_only = set(df_new.columns) - set(df_existing.columns)
            existing_only = set(df_existing.columns) - set(df_new.columns)
            if new_only or existing_only:
                all_cols = list(set(df_existing.columns) | set(df_new.columns))
                df_existing = df_existing.reindex(columns=all_cols)
                df_new = df_new.reindex(columns=all_cols)
            if 'order_number' in df_new.columns and 'order_number' in df_existing.columns:
                existing_orders = set(df_existing['order_number'].dropna())
                new_orders = set(df_new['order_number'].dropna())
                truly_new_orders = new_orders - existing_orders
                updated_orders = new_orders & existing_orders
                df_final = df_existing[~df_existing['order_number'].isin(updated_orders)].copy()
                df_final = pd.concat([df_final, df_new], ignore_index=True)
            else:
                df_final = pd.concat([df_existing, df_new], ignore_index=True)
        except Exception as e:
            print(f"âŒ è¯»å–ç°æœ‰ Parquet æ–‡ä»¶å¤±è´¥: {e}")
            print("   å°†ä»…ä½¿ç”¨æ–°æ•°æ®ã€‚")
            df_final = df_new
    else:
        print("ğŸ“ æœªå‘ç°ç°æœ‰ Parquet æ–‡ä»¶ï¼Œåˆ›å»ºæ–°æ–‡ä»¶...")
        df_final = df_new

    if 'parent_region_name' in df_final.columns:
        if df_final['parent_region_name'].nunique() < df_final.shape[0] * 0.5:
            df_final['parent_region_name'] = df_final['parent_region_name'].astype('category')
        else:
            df_final['parent_region_name'] = df_final['parent_region_name'].astype('string')

    # 5. æœ€ç»ˆå»é‡ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
    if 'order_number' in df_final.columns:
        print(f"âœ‚ï¸  æ‰§è¡Œæœ€ç»ˆå»é‡...")
        before_count = len(df_final)
        # keep='last' ç¡®ä¿ä¿ç•™æœ€ååŠ å…¥çš„è®°å½•ï¼ˆå³æœ€æ–°çš„ï¼‰
        df_final = df_final.drop_duplicates(subset=['order_number'], keep='last')
        after_count = len(df_final)
        print(f"   å»é‡å‰: {before_count}, å»é‡å: {after_count}, ç§»é™¤: {before_count - after_count}")

    # 6. ä¿å­˜
    if not FORMATTED_DIR.exists():
        FORMATTED_DIR.mkdir(parents=True)
    
    print(f"ğŸ’¾ ä¿å­˜åˆ°: {OUTPUT_FILE} ...")
    df_final.to_parquet(OUTPUT_FILE, index=False)
    
    # éªŒè¯
    if OUTPUT_FILE.exists():
        size_mb = OUTPUT_FILE.stat().st_size / (1024 * 1024)
        print(f"âœ… ä¿å­˜æˆåŠŸ! æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")
        print(f"   æœ€ç»ˆè¡Œæ•°: {df_final.shape[0]}")
    else:
        print("âŒ ä¿å­˜å¤±è´¥")

if __name__ == "__main__":
    main()
