import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import os
from datetime import datetime
import psutil
import json
from tableauhyperapi import HyperProcess, Telemetry, Connection, TableName

def get_memory_usage():
    """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def get_column_mapping():
    """
    è·å–ä¸­è‹±æ–‡å­—æ®µæ˜ å°„
    """
    column_mapping = {
        'æ—¥æœŸ': 'date',
        'å¹´æœˆ': 'year_month',
        'SUB_MODEL_ID': 'sub_model_id',
        'å­è½¦å‹': 'sub_model_name',
        'å“ç‰Œ': 'brand',
        'å‚å•†': 'manufacturer',
        'æ˜¯å¦æ ¸å¿ƒå“ç‰Œ': 'is_core_brand',
        'å“ç‰Œå±æ€§': 'brand_attribute',
        'é”€å”®çŠ¶æ€': 'sales_status',
        'å‹å·ç¼–ç ': 'model_code',
        'å‹å·ç®€ç§°': 'model_short_name',
        'å¹´å‹': 'model_year',
        'ä¸Šå¸‚æ—¶é—´': 'launch_date',
        'è½¦å‹': 'model_name',
        'ç»†åˆ†å¸‚åœº': 'segment',
        'ç»†åˆ†å¸‚åœº_ä¸Šæ±½': 'segment_saic',
        'ç»†åˆ†å¸‚åœº-è½¦èº«å½¢å¼': 'segment_body_style',
        'ç™¾å…¬é‡Œç”µè€—(kWh)': 'power_consumption_per_100km',
        'ç»­èˆªé‡Œç¨‹(km)': 'driving_range_km',
        'æ’é‡': 'displacement',
        'å……ç”µæ¡©è´¹ç”¨': 'charging_cost',
        'å˜é€Ÿç®±': 'transmission',
        'åº§ä½æ•°': 'seat_count',
        'è½¦èº«å½¢å¼': 'body_style',
        'è½´è·(mm)': 'wheelbase_mm',
        'è½´è·(Mm)': 'wheelbase_mm',
        'é•¿(mm)': 'length_mm',
        'é•¿(Mm)': 'length_mm',
        'å®½(mm)': 'width_mm',
        'å®½(Mm)': 'width_mm',
        'é«˜(mm)': 'height_mm',
        'é«˜(Mm)': 'height_mm',
        'Msrp': 'msrp',
        'MSRP': 'msrp',
        'TP': 'tp_avg',
        'Tp Avg': 'tp_avg',
        'TPé‡å¿ƒ': 'tp_center',
        'ç‡ƒæ–™ç§ç±»': 'fuel_type',
        'ç‡ƒæ–™ç±»å‹': 'fuel_type',
        'ç‡ƒæ–™ç±»å‹ (ç»„)': 'fuel_type_group',
        'æ˜¯å¦è±ªå': 'is_luxury',
        'æ˜¯å¦è±ªåå“ç‰Œ': 'is_luxury_brand',
        'æ˜¯å¦æ–°åŠ¿åŠ›å“ç‰Œ': 'is_new_energy_brand',
        'æ•´å¤‡è´¨é‡(kg)': 'curb_weight_kg',
        'é”€é‡': 'sales_volume',
        'ä¸Šé™©æ•°': 'insurance_volume',
        'çœ': 'province',
        'å¸‚': 'city',
        'åŸå¸‚çº§åˆ«': 'city_tier',
        'é™è´­/é™è¡Œ/åŒéé™': 'purchase_restriction',
        'æˆäº¤ä»·æ ¼': 'transaction_price',
        'æŒ‡å¯¼ä»·': 'msrp_price',
        'å±‚çº§': 'tier',
        'å±‚çº§ (ç»„)': 'tier_group',
        'TP 1ä¸‡1æ¡£': 'tp_1w_tier',
        'TP 5ä¸‡1æ¡£': 'tp_5w_tier',
        'TP 10ä¸‡1æ¡£': 'tp_10w_tier'
    }
    return column_mapping

def get_hyper_table_info(hyper_file_path):
    """
    è·å–Hyperæ–‡ä»¶çš„è¡¨ä¿¡æ¯å’Œè¡Œæ•°
    """
    with HyperProcess(telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU) as hyper:
        with Connection(endpoint=hyper.endpoint, database=hyper_file_path) as connection:
            # è·å–æ‰€æœ‰schema
            schemas = connection.catalog.get_schema_names()
            print(f"ğŸ“‹ å‘ç° {len(schemas)} ä¸ªschema: {schemas}")
            
            tables = []
            
            # éå†æ‰€æœ‰schemaæŸ¥æ‰¾è¡¨
            for schema in schemas:
                schema_tables = connection.catalog.get_table_names(schema)
                if schema_tables:
                    tables.extend([(schema, table) for table in schema_tables])
                    print(f"ğŸ“Š åœ¨schema '{schema}' ä¸­å‘ç° {len(schema_tables)} ä¸ªè¡¨: {[str(table) for table in schema_tables]}")
            
            if not tables:
                raise ValueError("Hyperæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨")
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªè¡¨
            schema_name, table_name = tables[0]
            print(f"ğŸ“Š åˆ†æè¡¨: {schema_name}.{table_name}")
            
            # è·å–è¡¨çš„è¡Œæ•°
            count_query = f"SELECT COUNT(*) FROM {table_name}"
            total_rows = connection.execute_scalar_query(count_query)
            
            # è·å–åˆ—åå¹¶æ¸…ç†åŒå¼•å·
            columns_info = connection.catalog.get_table_definition(table_name).columns
            column_names = [str(col.name).strip('"') for col in columns_info]
            
            print(f"ğŸ“Š è¡¨ä¿¡æ¯: {total_rows} è¡Œ x {len(column_names)} åˆ—")
            
            return table_name, column_names, total_rows

def read_hyper_file_batch(hyper_file_path, table_name, offset=0, limit=100000):
    """
    åˆ†æ‰¹è¯»å–Hyperæ–‡ä»¶å¹¶è¿”å›DataFrame
    """
    with HyperProcess(telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU) as hyper:
        with Connection(endpoint=hyper.endpoint, database=hyper_file_path) as connection:
            # æ‰§è¡Œåˆ†é¡µæŸ¥è¯¢
            query = f"SELECT * FROM {table_name} LIMIT {limit} OFFSET {offset}"
            result = connection.execute_list_query(query)
            
            # è·å–åˆ—åå¹¶æ¸…ç†åŒå¼•å·
            columns_info = connection.catalog.get_table_definition(table_name).columns
            column_names = [str(col.name).strip('"') for col in columns_info]
            
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(result, columns=column_names)
            
            return df

def analyze_column_types(df, sample_size=1000000):
    """åˆ†æDataFrameçš„åˆ—ç‰¹å¾ï¼Œå†³å®šæœ€ä½³æ•°æ®ç±»å‹"""
    print(f"ğŸ” åˆ†ææ•°æ®é›†ç‰¹å¾...")
    
    # å¦‚æœæ•°æ®é‡å¤ªå¤§ï¼Œåªåˆ†ææ ·æœ¬
    if len(df) > sample_size:
        sample_df = df.sample(n=sample_size)
    else:
        sample_df = df
    column_types = {}
    
    for col in sample_df.columns:
        col_str = str(col)  # ç¡®ä¿åˆ—åæ˜¯å­—ç¬¦ä¸²
        if sample_df[col_str].dtype == 'object':
            unique_count = sample_df[col_str].nunique()
            total_count = len(sample_df[col_str])
            
            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
            numeric_series = pd.to_numeric(sample_df[col_str], errors='coerce')
            numeric_ratio = numeric_series.notna().sum() / total_count
            
            if numeric_ratio > 0.8:  # 80%ä»¥ä¸Šå¯ä»¥è½¬ä¸ºæ•°å€¼
                column_types[col_str] = 'numeric'
                print(f"  {col_str}: {unique_count} å”¯ä¸€å€¼, æ•°å€¼æ¯”ä¾‹ {numeric_ratio:.2%} -> numeric")
            else:
                # è½¬æ¢ä¸ºcategoryçš„æ¡ä»¶
                column_types[col_str] = 'category'
                print(f"  {col_str}: {unique_count} å”¯ä¸€å€¼, æ•°å€¼æ¯”ä¾‹ {numeric_ratio:.2%} -> category")
        else:
            column_types[col_str] = 'keep'  # ä¿æŒåŸç±»å‹
    
    return column_types

def hyper_to_parquet_optimized(hyper_file_path, parquet_output_path, batch_size=100000):
    """
    å°†Hyperæ–‡ä»¶åˆ†æ‰¹è½¬æ¢ä¸ºä¼˜åŒ–çš„Parquetæ–‡ä»¶
    """
    try:
        print(f"ğŸ”„ æ­£åœ¨åˆ†æHyperæ–‡ä»¶: {hyper_file_path}")
        
        # è·å–è¡¨ä¿¡æ¯
        table_name, column_names, total_rows = get_hyper_table_info(hyper_file_path)
        
        print(f"ğŸ“Š æ•°æ®æ€»é‡: {total_rows:,} è¡Œï¼Œå°†åˆ† {(total_rows + batch_size - 1) // batch_size} æ‰¹å¤„ç†")
        print(f"ğŸ“¦ æ¯æ‰¹å¤„ç†: {batch_size:,} è¡Œ")
        
        # è¯»å–ç¬¬ä¸€æ‰¹æ•°æ®ç”¨äºåˆ†æåˆ—ç±»å‹å’Œè®¾ç½®schema
        print("\nğŸ” è¯»å–æ ·æœ¬æ•°æ®åˆ†æåˆ—ç±»å‹...")
        sample_df = read_hyper_file_batch(hyper_file_path, table_name, 0, min(batch_size, total_rows))
        
        # åˆ†æåˆ—ç±»å‹
        column_types = analyze_column_types(sample_df)
        
        # åº”ç”¨ä¸­è‹±æ–‡å­—æ®µæ˜ å°„
        column_mapping = get_column_mapping()
        original_columns = sample_df.columns.tolist()
        
        print("\nğŸ”„ åº”ç”¨ä¸­è‹±æ–‡å­—æ®µæ˜ å°„...")
        # åˆ›å»ºæ˜ å°„åçš„åˆ—å
        new_columns = []
        column_name_mapping = {}  # ä¿å­˜åŸå§‹åˆ—ååˆ°è‹±æ–‡åˆ—åçš„æ˜ å°„
        
        for col in sample_df.columns:
            if col in column_mapping:
                new_col = column_mapping[col]
                column_name_mapping[col] = new_col
                new_columns.append(new_col)
                print(f"  {col} -> {new_col}")
            else:
                # å¦‚æœæ²¡æœ‰æ˜ å°„ï¼Œä¿æŒåŸåä½†è½¬ä¸ºå°å†™å¹¶æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
                new_col = col.lower().replace('(', '_').replace(')', '').replace(' ', '_').replace('-', '_')
                column_name_mapping[col] = new_col
                new_columns.append(new_col)
                print(f"  {col} -> {new_col} (è‡ªåŠ¨è½¬æ¢)")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(parquet_output_path)
        os.makedirs(output_dir, exist_ok=True)
        
        # å¤„ç†ç¬¬ä¸€æ‰¹æ•°æ®ä»¥ç¡®å®šschema
        print(f"\nğŸš€ å¼€å§‹åˆ†æ‰¹å¤„ç†æ•°æ®...")
        
        # åº”ç”¨æ•°æ®ç±»å‹ä¼˜åŒ–åˆ°æ ·æœ¬æ•°æ®ï¼ˆè·³è¿‡categoryç±»å‹é¿å…ç¼–ç å†²çªï¼‰
        for col, target_type in column_types.items():
            col_str = str(col)
            if col_str in sample_df.columns:
                if target_type == 'numeric':
                    sample_df[col_str] = pd.to_numeric(sample_df[col_str], errors='coerce')
                # è·³è¿‡categoryç±»å‹ï¼Œä¿æŒä¸ºobjectä»¥é¿å…ä¸åŒæ‰¹æ¬¡é—´çš„ç¼–ç å†²çª
        
        # ä¼˜åŒ–æ—¥æœŸåˆ—
        for col in sample_df.columns:
            col_str = str(col)
            if 'æ—¥æœŸ' in col_str or 'date' in col_str.lower():
                try:
                    sample_df[col_str] = pd.to_datetime(sample_df[col_str], errors='coerce')
                except:
                    pass
        
        # é‡å‘½ååˆ—
        sample_df.columns = new_columns
        
        # è·å–ç»Ÿä¸€çš„schema
        unified_schema = pa.Table.from_pandas(sample_df).schema
        print(f"ğŸ“‹ ç¡®å®šç»Ÿä¸€schema: {len(unified_schema)} ä¸ªå­—æ®µ")
        
        # åˆ›å»ºParquet writer
        parquet_writer = pq.ParquetWriter(parquet_output_path, unified_schema, compression='snappy')
        
        # å†™å…¥ç¬¬ä¸€æ‰¹æ•°æ®
        first_table = pa.Table.from_pandas(sample_df, schema=unified_schema)
        parquet_writer.write_table(first_table)
        processed_rows = len(sample_df)
        
        print(f"\nğŸ“¦ å¤„ç†ç¬¬ 1/{(total_rows + batch_size - 1) // batch_size} æ‰¹ (è¡Œ 1 - {len(sample_df):,})")
        memory_usage = get_memory_usage()
        progress = (processed_rows / total_rows) * 100
        print(f"âœ… å·²å¤„ç† {processed_rows:,}/{total_rows:,} è¡Œ ({progress:.1f}%) | å†…å­˜: {memory_usage:.1f} MB")
        
        # æ¸…ç†ç¬¬ä¸€æ‰¹æ•°æ®
        del first_table
        
        # å¤„ç†å‰©ä½™æ‰¹æ¬¡
        for batch_num in range(batch_size, total_rows, batch_size):
            current_batch = (batch_num // batch_size) + 1
            total_batches = (total_rows + batch_size - 1) // batch_size
            
            print(f"\nğŸ“¦ å¤„ç†ç¬¬ {current_batch}/{total_batches} æ‰¹ (è¡Œ {batch_num+1:,} - {min(batch_num + batch_size, total_rows):,})")
            
            # è¯»å–å½“å‰æ‰¹æ¬¡æ•°æ®
            df_batch = read_hyper_file_batch(hyper_file_path, table_name, batch_num, batch_size)
            
            if len(df_batch) == 0:
                print("âš ï¸ å½“å‰æ‰¹æ¬¡æ— æ•°æ®ï¼Œè·³è¿‡")
                continue
            
            # åº”ç”¨æ•°æ®ç±»å‹ä¼˜åŒ–ï¼ˆè·³è¿‡categoryç±»å‹é¿å…ç¼–ç å†²çªï¼‰
            for col, target_type in column_types.items():
                col_str = str(col)
                if col_str in df_batch.columns:
                    if target_type == 'numeric':
                        df_batch[col_str] = pd.to_numeric(df_batch[col_str], errors='coerce')
                    # è·³è¿‡categoryç±»å‹ï¼Œä¿æŒä¸ºobjectä»¥é¿å…ä¸åŒæ‰¹æ¬¡é—´çš„ç¼–ç å†²çª
            
            # ä¼˜åŒ–æ—¥æœŸåˆ—
            for col in df_batch.columns:
                col_str = str(col)
                if 'æ—¥æœŸ' in col_str or 'date' in col_str.lower():
                    try:
                        df_batch[col_str] = pd.to_datetime(df_batch[col_str], errors='coerce')
                    except:
                        pass
            
            # é‡å‘½ååˆ—
            df_batch.columns = new_columns
            
            # è½¬æ¢ä¸ºPyArrowè¡¨ï¼Œä½¿ç”¨ç»Ÿä¸€schema
            table = pa.Table.from_pandas(df_batch, schema=unified_schema)
            
            # å†™å…¥Parquetæ–‡ä»¶
            parquet_writer.write_table(table)
            processed_rows += len(df_batch)
            
            # æ˜¾ç¤ºè¿›åº¦å’Œå†…å­˜ä½¿ç”¨
            memory_usage = get_memory_usage()
            progress = (processed_rows / total_rows) * 100
            print(f"âœ… å·²å¤„ç† {processed_rows:,}/{total_rows:,} è¡Œ ({progress:.1f}%) | å†…å­˜: {memory_usage:.1f} MB")
            
            # æ¸…ç†å†…å­˜
            del df_batch, table
        
        # å…³é—­writer
        if parquet_writer:
            parquet_writer.close()
        
        # éªŒè¯ä¿å­˜ç»“æœ
        file_size = os.path.getsize(parquet_output_path) / (1024 * 1024)  # MB
        memory_usage = get_memory_usage()
        print(f"\nâœ… è½¬æ¢å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {parquet_output_path}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        print(f"ğŸ’¾ æœ€ç»ˆå†…å­˜ä½¿ç”¨: {memory_usage:.2f} MB")
        
        # ç”Ÿæˆschema.jsonæ–‡ä»¶ï¼ˆä½¿ç”¨æ ·æœ¬æ•°æ®ï¼‰
        # æ³¨æ„ï¼šsample_dfçš„åˆ—åå·²ç»åœ¨å‰é¢è¢«ä¿®æ”¹ä¸ºnew_columnsäº†
        generate_schema_json(sample_df, parquet_output_path, original_columns, column_name_mapping)
        
        return True
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def generate_schema_json(df, parquet_output_path, original_columns, column_name_mapping):
    """
    ç”Ÿæˆæ•°æ®schemaçš„JSONæ–‡ä»¶ï¼ŒåŒ…å«ä¸­è‹±æ–‡å­—æ®µæ˜ å°„
    """
    # åŸºç¡€æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    base_name = os.path.splitext(os.path.basename(parquet_output_path))[0]
    schema_path = os.path.join(os.path.dirname(parquet_output_path), f"{base_name}_schema.json")
    
    # å­—æ®µç±»å‹æ˜ å°„
    dtype_mapping = {
        'object': 'æ–‡æœ¬',
        'category': 'åˆ†ç±»',
        'int64': 'æ•´æ•°',
        'int32': 'æ•´æ•°',
        'int16': 'æ•´æ•°',
        'int8': 'æ•´æ•°',
        'uint64': 'æ— ç¬¦å·æ•´æ•°',
        'uint32': 'æ— ç¬¦å·æ•´æ•°',
        'uint16': 'æ— ç¬¦å·æ•´æ•°',
        'uint8': 'æ— ç¬¦å·æ•´æ•°',
        'float64': 'æµ®ç‚¹æ•°',
        'float32': 'æµ®ç‚¹æ•°',
        'datetime64[ns]': 'æ—¥æœŸæ—¶é—´',
        'bool': 'å¸ƒå°”å€¼'
    }
    
    # å­—æ®µæ³¨é‡Šæ˜ å°„ï¼ˆåŸºäºå¸¸è§å­—æ®µåï¼‰
    field_comments = {
        'æ—¥æœŸ': 'æ•°æ®è®°å½•æ—¥æœŸ',
        'å¹´æœˆ': 'å¹´æœˆä¿¡æ¯',
        'å“ç‰Œ': 'æ±½è½¦å“ç‰Œåç§°',
        'å“ç‰Œï¼ˆæ–°ï¼‰': 'æ›´æ–°åçš„å“ç‰Œåˆ†ç±»',
        'å‚å•†': 'æ±½è½¦åˆ¶é€ å‚å•†',
        'è½¦ç³»': 'è½¦å‹ç³»åˆ—',
        'è½¦å‹': 'å…·ä½“è½¦å‹åç§°',
        'å­è½¦å‹': 'ç»†åˆ†è½¦å‹',
        'è½¦èº«å½¢å¼': 'è½¦èº«ç±»å‹ï¼ˆå¦‚SUVã€è½¿è½¦ç­‰ï¼‰',
        'ç‡ƒæ–™ç§ç±»': 'ç‡ƒæ–™ç±»å‹ï¼ˆå¦‚æ±½æ²¹ã€ç”µåŠ¨ç­‰ï¼‰',
        'å±‚çº§': 'è½¦å‹å±‚çº§åˆ†ç±»',
        'å±‚çº§ (ç»„)': 'è½¦å‹å±‚çº§åˆ†ç»„',
        'çœ': 'çœä»½',
        'å¸‚': 'åŸå¸‚',
        'åŸå¸‚çº§åˆ«': 'åŸå¸‚ç­‰çº§åˆ†ç±»',
        'é™è´­/é™è¡Œ/åŒéé™': 'åŸå¸‚é™è´­é™è¡Œæ”¿ç­–',
        'ä¸Šé™©æ•°': 'è½¦è¾†ä¸Šä¿é™©æ•°é‡',
        'é”€é‡': 'é”€å”®æ•°é‡',
        'æˆäº¤ä»·æ ¼': 'å®é™…æˆäº¤ä»·æ ¼',
        'TP': 'æˆäº¤ä»·æ ¼',
        'æŒ‡å¯¼ä»·': 'å‚å•†æŒ‡å¯¼ä»·æ ¼',
        'é•¿(mm)': 'è½¦èº«é•¿åº¦ï¼ˆæ¯«ç±³ï¼‰',
        'å®½(mm)': 'è½¦èº«å®½åº¦ï¼ˆæ¯«ç±³ï¼‰',
        'é«˜(mm)': 'è½¦èº«é«˜åº¦ï¼ˆæ¯«ç±³ï¼‰',
        'è½´è·(mm)': 'è½´è·é•¿åº¦ï¼ˆæ¯«ç±³ï¼‰'
    }
    
    # æ„å»ºschemaç»“æ„
    schema = {
        base_name: {
            'description': 'è½¦è¾†ä¸Šé™©é‡æ•°æ®',
            'columns': list(df.columns),  # è‹±æ–‡åˆ—å
            'original_columns': original_columns,  # åŸå§‹ä¸­æ–‡åˆ—å
            'column_mapping': column_name_mapping,  # ä¸­è‹±æ–‡æ˜ å°„å…³ç³»
            'column_explanations': {},
            'column_types': {},
            'value_mappings': {},
            'primary_metrics': [],
            'date_column': '',
            'metadata': {
                'time_granularity': 'monthly',
                'geo_dimension': 'province_city',
                'brand_dimension': 'brand',
                'notes': 'è½¦è¾†ä¸Šé™©é‡æœˆåº¦ç»Ÿè®¡æ•°æ®ï¼ŒåŒ…å«å“ç‰Œã€è½¦å‹ã€åœ°åŒºç­‰ç»´åº¦ä¿¡æ¯',
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'file_size_mb': round(os.path.getsize(parquet_output_path) / (1024 * 1024), 2)
            }
        }
    }
    
    # å¡«å……å­—æ®µä¿¡æ¯
    for col in df.columns:
        # æ•°æ®ç±»å‹
        dtype_str = str(df[col].dtype)
        schema[base_name]['column_types'][col] = dtype_mapping.get(dtype_str, dtype_str)
        
        # å­—æ®µæ³¨é‡Š - åŸºäºåŸå§‹ä¸­æ–‡åˆ—åè·å–æ³¨é‡Š
        original_col = None
        for orig_col, mapped_col in column_name_mapping.items():
            if mapped_col == col:
                original_col = orig_col
                break
        
        if original_col:
            schema[base_name]['column_explanations'][col] = field_comments.get(original_col, f'{original_col}å­—æ®µ')
        else:
            schema[base_name]['column_explanations'][col] = f'{col}å­—æ®µ'
        
        # è¯†åˆ«æ—¥æœŸåˆ—
        if 'date' in col.lower() or 'year_month' in col.lower():
            schema[base_name]['date_column'] = col
        
        # è¯†åˆ«ä¸»è¦æŒ‡æ ‡
        if any(keyword in col for keyword in ['insurance_volume', 'sales_volume', 'volume']):
            schema[base_name]['primary_metrics'].append(col)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦æŒ‡æ ‡ï¼Œæ·»åŠ é»˜è®¤çš„
    if not schema[base_name]['primary_metrics']:
        numeric_cols = [col for col in df.columns if df[col].dtype in ['int64', 'float64']]
        if numeric_cols:
            schema[base_name]['primary_metrics'] = numeric_cols[:3]  # å–å‰3ä¸ªæ•°å€¼åˆ—
    
    # ä¿å­˜schemaæ–‡ä»¶
    try:
        with open(schema_path, 'w', encoding='utf-8') as f:
            json.dump(schema, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“‹ Schemaæ–‡ä»¶å·²ç”Ÿæˆ: {schema_path}")
    except Exception as e:
        print(f"âš ï¸ Schemaæ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")

if __name__ == "__main__":
    hyper_file = "/Users/zihao_/Documents/coding/dataset/original/ä¹˜ç”¨è½¦ä¸Šé™©é‡_0826.hyper"
    parquet_file = "/Users/zihao_/Documents/coding/dataset/formatted/ä¹˜ç”¨è½¦ä¸Šé™©é‡_0826.parquet"
    
    # æ‰¹å¤„ç†å¤§å°é…ç½®ï¼ˆå¯æ ¹æ®å†…å­˜æƒ…å†µè°ƒæ•´ï¼‰
    batch_size = 100000  # æ¯æ‰¹å¤„ç†10ä¸‡è¡Œ
    
    # æ£€æŸ¥Hyperæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(hyper_file):
        print(f"âŒ é”™è¯¯: Hyperæ–‡ä»¶ä¸å­˜åœ¨: {hyper_file}")
        exit(1)
    
    print("ğŸš€ å¼€å§‹å¤§æ•°æ®é›†Hyperåˆ°Parquetåˆ†æ‰¹è½¬æ¢...")
    print("=" * 60)
    print(f"ğŸ“¦ æ‰¹å¤„ç†é…ç½®: æ¯æ‰¹ {batch_size:,} è¡Œ")
    print(f"ğŸ’¾ å†…å­˜ä¼˜åŒ–: åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡º")
    print("=" * 60)
    
    success = hyper_to_parquet_optimized(hyper_file, parquet_file, batch_size)
    
    if success:
        print("\nğŸ‰ å¤§æ•°æ®é›†è½¬æ¢æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {parquet_file}")
        print(f"ğŸ”§ ä¼˜åŒ–æ•ˆæœ: å†…å­˜ä½¿ç”¨æ§åˆ¶åœ¨åˆç†èŒƒå›´å†…")
    else:
        print("\nâŒ è½¬æ¢å¤±è´¥")
        exit(1)