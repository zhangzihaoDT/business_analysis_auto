#!/usr/bin/env python3
"""
CM2é…ç½®é¡¹å½±å“åˆ†æå»ºæ¨¡è„šæœ¬

åŠŸèƒ½ï¼š
1. æ•°æ®ç­›é€‰ï¼šlock_time >= 2025-09-10 ä¸” invoice_time ä¸ä¸ºç©º
2. çº¿æ€§å›å½’å’ŒXGBoostå›å½’åˆ†æé…ç½®é¡¹å¯¹å¼€ç¥¨ä»·æ ¼çš„å½±å“
3. åˆ†æé…ç½®é¡¹å¯¹é”€é‡çš„å½±å“
4. ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–ç»“æœ

ä½œè€…ï¼šAI Assistant
åˆ›å»ºæ—¶é—´ï¼š2025-10-23
"""

import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.io as pio
import warnings
from datetime import datetime
import argparse
import os
import sys
from pathlib import Path

# æœºå™¨å­¦ä¹ ç›¸å…³åº“
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb

# è®¾ç½®Plotlyé»˜è®¤ä¸»é¢˜å’Œä¸­æ–‡æ”¯æŒ
pio.templates.default = "plotly_white"
warnings.filterwarnings('ignore')

class CM2ConfigurationAnalyzer:
    """CM2é…ç½®é¡¹å½±å“åˆ†æå™¨"""
    
    def __init__(self, data_path, output_dir=None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.data_path = data_path
        self.output_dir = output_dir or os.path.join(os.path.dirname(data_path), 'analysis_results')
        self.raw_data = None
        self.filtered_data = None
        self.feature_columns = ['Product Name', 'Product_Types', 'EXCOLOR', 'INCOLOR', 'OP-FRIDGE', 'OP-LASER', 'OP-LuxGift', 'OP-SW', 'WHEEL']
        self.target_column = 'å¼€ç¥¨ä»·æ ¼'
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"é…ç½®é¡¹å½±å“åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"æ•°æ®æ–‡ä»¶: {self.data_path}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_and_filter_data(self):
        """åŠ è½½å¹¶ç­›é€‰æ•°æ®"""
        print("\n=== æ•°æ®åŠ è½½å’Œç­›é€‰ ===")
        
        # åŠ è½½æ•°æ®
        print(f"æ­£åœ¨åŠ è½½æ•°æ®: {self.data_path}")
        self.raw_data = pd.read_csv(self.data_path)
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {self.raw_data.shape}")
        
        # è½¬æ¢æ—¥æœŸå­—æ®µ
        self.raw_data['lock_time'] = pd.to_datetime(self.raw_data['lock_time'])
        
        # æ•°æ®ç­›é€‰
        print("åº”ç”¨ç­›é€‰æ¡ä»¶:")
        print("1. lock_time >= 2025-09-10")
        print("2. invoice_time ä¸ä¸ºç©º")
        
        # ç­›é€‰æ¡ä»¶
        condition = (
            (self.raw_data['lock_time'] >= '2025-09-10') &
            (self.raw_data['invoice_time'].notna())
        )
        
        self.filtered_data = self.raw_data[condition].copy()
        print(f"ç­›é€‰åæ•°æ®å½¢çŠ¶: {self.filtered_data.shape}")
        print(f"ç­›é€‰ä¿ç•™æ¯”ä¾‹: {len(self.filtered_data)/len(self.raw_data)*100:.2f}%")
        
        # æ•°æ®åŸºæœ¬ä¿¡æ¯
        print(f"\nç­›é€‰åæ•°æ®æ¦‚è§ˆ:")
        print(f"- æ—¶é—´èŒƒå›´: {self.filtered_data['lock_time'].min()} è‡³ {self.filtered_data['lock_time'].max()}")
        print(f"- å¼€ç¥¨ä»·æ ¼èŒƒå›´: {self.filtered_data[self.target_column].min():,.0f} - {self.filtered_data[self.target_column].max():,.0f}")
        print(f"- ä¸åŒè®¢å•æ•°é‡: {self.filtered_data['order_number'].nunique():,}")
        
        return self.filtered_data
    
    def prepare_features(self):
        """å‡†å¤‡ç‰¹å¾æ•°æ®"""
        print("\n=== ç‰¹å¾æ•°æ®å‡†å¤‡ ===")
        
        # æ£€æŸ¥ç‰¹å¾åˆ—
        missing_features = [col for col in self.feature_columns if col not in self.filtered_data.columns]
        if missing_features:
            raise ValueError(f"ç¼ºå°‘ç‰¹å¾åˆ—: {missing_features}")
        
        # åˆ›å»ºç‰¹å¾æ•°æ®å‰¯æœ¬
        feature_data = self.filtered_data[self.feature_columns + [self.target_column]].copy()
        
        # å¤„ç†åˆ†ç±»ç‰¹å¾
        label_encoders = {}
        for col in self.feature_columns:
            if feature_data[col].dtype == 'object':
                le = LabelEncoder()
                feature_data[f'{col}_encoded'] = le.fit_transform(feature_data[col].astype(str))
                label_encoders[col] = le
                print(f"å¯¹ {col} è¿›è¡Œæ ‡ç­¾ç¼–ç ï¼Œç±»åˆ«æ•°: {len(le.classes_)}")
            else:
                feature_data[f'{col}_encoded'] = feature_data[col]
                print(f"{col} å·²ä¸ºæ•°å€¼ç±»å‹ï¼Œæ— éœ€ç¼–ç ")
        
        # è·å–ç¼–ç åçš„ç‰¹å¾åˆ—
        encoded_features = [f'{col}_encoded' for col in self.feature_columns]
        
        # ç‰¹å¾ç»Ÿè®¡
        print(f"\nç‰¹å¾ç»Ÿè®¡:")
        for col in self.feature_columns:
            print(f"- {col}: {feature_data[col].value_counts().to_dict()}")
        
        self.feature_data = feature_data
        self.encoded_features = encoded_features
        self.label_encoders = label_encoders
        
        return feature_data, encoded_features
    
    def linear_regression_analysis(self):
        """çº¿æ€§å›å½’åˆ†æ"""
        print("\n=== çº¿æ€§å›å½’åˆ†æ ===")
        
        # å‡†å¤‡æ•°æ®
        X = self.feature_data[self.encoded_features]
        y = self.feature_data[self.target_column]
        
        # æ•°æ®æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )
        
        # è®­ç»ƒæ¨¡å‹
        lr_model = LinearRegression()
        lr_model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred_train = lr_model.predict(X_train)
        y_pred_test = lr_model.predict(X_test)
        
        # è¯„ä¼°æŒ‡æ ‡
        train_r2 = r2_score(y_train, y_pred_train)
        test_r2 = r2_score(y_test, y_pred_test)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
        train_mae = mean_absolute_error(y_train, y_pred_train)
        test_mae = mean_absolute_error(y_test, y_pred_test)
        
        print(f"çº¿æ€§å›å½’æ¨¡å‹æ€§èƒ½:")
        print(f"è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
        print(f"æµ‹è¯•é›† RÂ²: {test_r2:.4f}")
        print(f"è®­ç»ƒé›† RMSE: {train_rmse:,.0f}")
        print(f"æµ‹è¯•é›† RMSE: {test_rmse:,.0f}")
        print(f"è®­ç»ƒé›† MAE: {train_mae:,.0f}")
        print(f"æµ‹è¯•é›† MAE: {test_mae:,.0f}")
        
        # ç‰¹å¾é‡è¦æ€§ï¼ˆç³»æ•°ï¼‰
        feature_importance = pd.DataFrame({
            'feature': self.feature_columns,
            'coefficient': lr_model.coef_,
            'abs_coefficient': np.abs(lr_model.coef_)
        }).sort_values('abs_coefficient', ascending=False)
        
        print(f"\nçº¿æ€§å›å½’ç‰¹å¾é‡è¦æ€§ï¼ˆç³»æ•°ï¼‰:")
        for _, row in feature_importance.iterrows():
            print(f"- {row['feature']}: {row['coefficient']:,.2f}")
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(lr_model, X_scaled, y, cv=5, scoring='r2')
        print(f"\n5æŠ˜äº¤å‰éªŒè¯ RÂ² åˆ†æ•°: {cv_scores.mean():.4f} (Â±{cv_scores.std()*2:.4f})")
        
        # ä¿å­˜ç»“æœ
        self.lr_results = {
            'model': lr_model,
            'scaler': scaler,
            'feature_importance': feature_importance,
            'metrics': {
                'train_r2': train_r2,
                'test_r2': test_r2,
                'train_rmse': train_rmse,
                'test_rmse': test_rmse,
                'train_mae': train_mae,
                'test_mae': test_mae,
                'cv_r2_mean': cv_scores.mean(),
                'cv_r2_std': cv_scores.std()
            },
            'predictions': {
                'y_train': y_train,
                'y_test': y_test,
                'y_pred_train': y_pred_train,
                'y_pred_test': y_pred_test
            }
        }
        
        return self.lr_results
    
    def xgboost_regression_analysis(self):
        """XGBoostå›å½’åˆ†æ"""
        print("\n=== XGBoostå›å½’åˆ†æ ===")
        
        # å‡†å¤‡æ•°æ®
        X = self.feature_data[self.encoded_features]
        y = self.feature_data[self.target_column]
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # è®­ç»ƒXGBoostæ¨¡å‹
        xgb_model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            n_jobs=-1
        )
        
        xgb_model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred_train = xgb_model.predict(X_train)
        y_pred_test = xgb_model.predict(X_test)
        
        # è¯„ä¼°æŒ‡æ ‡
        train_r2 = r2_score(y_train, y_pred_train)
        test_r2 = r2_score(y_test, y_pred_test)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
        train_mae = mean_absolute_error(y_train, y_pred_train)
        test_mae = mean_absolute_error(y_test, y_pred_test)
        
        print(f"XGBoostæ¨¡å‹æ€§èƒ½:")
        print(f"è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
        print(f"æµ‹è¯•é›† RÂ²: {test_r2:.4f}")
        print(f"è®­ç»ƒé›† RMSE: {train_rmse:,.0f}")
        print(f"æµ‹è¯•é›† RMSE: {test_rmse:,.0f}")
        print(f"è®­ç»ƒé›† MAE: {train_mae:,.0f}")
        print(f"æµ‹è¯•é›† MAE: {test_mae:,.0f}")
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': xgb_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"\nXGBoostç‰¹å¾é‡è¦æ€§:")
        for _, row in feature_importance.iterrows():
            print(f"- {row['feature']}: {row['importance']:.4f}")
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(xgb_model, X, y, cv=5, scoring='r2')
        print(f"\n5æŠ˜äº¤å‰éªŒè¯ RÂ² åˆ†æ•°: {cv_scores.mean():.4f} (Â±{cv_scores.std()*2:.4f})")
        
        # ä¿å­˜ç»“æœ
        self.xgb_results = {
            'model': xgb_model,
            'feature_importance': feature_importance,
            'metrics': {
                'train_r2': train_r2,
                'test_r2': test_r2,
                'train_rmse': train_rmse,
                'test_rmse': test_rmse,
                'train_mae': train_mae,
                'test_mae': test_mae,
                'cv_r2_mean': cv_scores.mean(),
                'cv_r2_std': cv_scores.std()
            },
            'predictions': {
                'y_train': y_train,
                'y_test': y_test,
                'y_pred_train': y_pred_train,
                'y_pred_test': y_pred_test
            }
        }
        
        return self.xgb_results
    
    def sales_volume_analysis(self):
        """é”€é‡åˆ†æï¼ˆæŒ‰é…ç½®é¡¹ç»Ÿè®¡è®¢å•æ•°é‡ï¼‰"""
        print("\n=== é”€é‡åˆ†æ ===")
        
        # æ€»ä½“é”€é‡
        total_orders = len(self.filtered_data)
        unique_orders = self.filtered_data['order_number'].nunique()
        print(f"æ€»è®¢å•è®°å½•æ•°: {total_orders:,}")
        print(f"å”¯ä¸€è®¢å•æ•°: {unique_orders:,}")
        
        # æŒ‰é…ç½®é¡¹åˆ†æé”€é‡
        sales_analysis = {}
        
        for feature in self.feature_columns:
            print(f"\n--- {feature} é”€é‡åˆ†æ ---")
            
            # æŒ‰é…ç½®é¡¹å€¼ç»Ÿè®¡è®¢å•æ•°é‡
            feature_sales = self.filtered_data.groupby(feature).agg({
                'order_number': 'count',  # è®¢å•è®°å½•æ•°
                'order_number': 'nunique'  # å”¯ä¸€è®¢å•æ•°
            }).rename(columns={'order_number': 'unique_orders'})
            
            # é‡æ–°è®¡ç®—ï¼ˆå› ä¸ºä¸Šé¢çš„èšåˆæœ‰é—®é¢˜ï¼‰
            feature_sales = self.filtered_data.groupby(feature).agg({
                'order_number': ['count', 'nunique'],
                self.target_column: ['mean', 'median', 'std']
            })
            
            # æ‰å¹³åŒ–åˆ—å
            feature_sales.columns = ['_'.join(col).strip() for col in feature_sales.columns]
            feature_sales = feature_sales.rename(columns={
                'order_number_count': 'total_records',
                'order_number_nunique': 'unique_orders',
                f'{self.target_column}_mean': 'avg_price',
                f'{self.target_column}_median': 'median_price',
                f'{self.target_column}_std': 'price_std'
            })
            
            # è®¡ç®—å¸‚åœºä»½é¢
            feature_sales['market_share'] = feature_sales['unique_orders'] / unique_orders * 100
            
            # æ’åº
            feature_sales = feature_sales.sort_values('unique_orders', ascending=False)
            
            print(feature_sales.round(2))
            
            sales_analysis[feature] = feature_sales
        
        self.sales_analysis = sales_analysis
        return sales_analysis
    
    def calculate_configuration_contribution(self):
        """è®¡ç®—é…ç½®é¡¹å¯¹æˆäº¤ä»·å’Œé”€é‡çš„è´¡çŒ®"""
        print("\n=== è®¡ç®—é…ç½®æ”¶ç›Šè´¡çŒ® ===")
        
        contribution_data = []
        
        for feature in self.feature_columns:
            # è·å–çº¿æ€§å›å½’ç³»æ•°ï¼ˆä»·æ ¼è´¡çŒ®ï¼‰
            lr_coef = self.lr_results['feature_importance'][
                self.lr_results['feature_importance']['feature'] == feature
            ]['coefficient'].iloc[0]
            
            # è·å–XGBoosté‡è¦æ€§
            xgb_importance = self.xgb_results['feature_importance'][
                self.xgb_results['feature_importance']['feature'] == feature
            ]['importance'].iloc[0]
            
            # è®¡ç®—é”€é‡å½±å“ï¼ˆåŸºäºä¸åŒé…ç½®é€‰é¡¹çš„å¸‚åœºä»½é¢å·®å¼‚ï¼‰
            sales_data = self.sales_analysis[feature]
            if len(sales_data) > 1:
                # è®¡ç®—æœ€é«˜ä»½é¢ä¸æœ€ä½ä»½é¢çš„å·®å¼‚
                max_share = sales_data['market_share'].max()
                min_share = sales_data['market_share'].min()
                sales_impact = max_share - min_share
                
                # è®¡ç®—å¹³å‡ä»·æ ¼å·®å¼‚
                max_price_option = sales_data.loc[sales_data['avg_price'].idxmax()]
                min_price_option = sales_data.loc[sales_data['avg_price'].idxmin()]
                price_diff = max_price_option['avg_price'] - min_price_option['avg_price']
            else:
                sales_impact = 0
                price_diff = 0
            
            # ç”Ÿæˆç»“è®º
            if lr_coef > 5000 and sales_impact > 10:
                conclusion = "æ˜æ˜¾å—æ¬¢è¿ï¼Œå»ºè®®æ ‡é…"
            elif lr_coef > 3000 and sales_impact > 5:
                conclusion = "ç¨³å®šå¢ç›Šé¡¹"
            elif lr_coef > 1000 and sales_impact < 5:
                conclusion = "æä»·å¤šä½†ä¸å¸¦åŠ¨é”€é‡"
            elif lr_coef < 1000 and sales_impact > 10:
                conclusion = "å¼ºæ‹‰åŠ¨ï¼Œå»ºè®®ä¼˜åŒ–ç»­èˆª" if "ç”µæ± " in feature else "å¼ºæ‹‰åŠ¨ï¼Œå»ºè®®æ¨å¹¿"
            elif abs(lr_coef) < 500 and abs(sales_impact) < 2:
                conclusion = "ä¸­æ€§é…ç½®ï¼Œå¯è§†æˆæœ¬ä¼˜åŒ–"
            elif lr_coef > 0 and sales_impact > 0:
                conclusion = "å¼±æ­£å‘ï¼Œå¯ç•™åœ¨é«˜é…"
            else:
                conclusion = "å½±å“è¾ƒå°ï¼Œå¯è€ƒè™‘ç®€åŒ–"
            
            contribution_data.append({
                'feature': feature,
                'price_contribution': lr_coef / 10000,  # è½¬æ¢ä¸ºä¸‡å…ƒ
                'sales_impact': sales_impact,
                'xgb_importance': xgb_importance,
                'conclusion': conclusion,
                'price_diff': price_diff
            })
        
        self.contribution_df = pd.DataFrame(contribution_data).sort_values(
            'xgb_importance', ascending=False
        )
        
        return self.contribution_df
    
    def generate_price_elasticity_analysis(self):
        """ç”Ÿæˆä»·æ ¼-é”€é‡å“åº”æ›²çº¿åˆ†æ"""
        print("\n=== ä»·æ ¼å¼¹æ€§åˆ†æ ===")
        
        # åŸºäºç°æœ‰æ•°æ®ç”Ÿæˆä¸åŒé…ç½®ç»„åˆçš„ä»·æ ¼-é”€é‡é¢„æµ‹
        base_price = self.filtered_data[self.target_column].median()
        
        # å®šä¹‰å‡ ç§å…¸å‹é…ç½®ç»„åˆ
        config_scenarios = {
            'åŸºç¡€ç‰ˆ': {'OP-FRIDGE': 0, 'WHEEL': 0, 'OP-LuxGift': 0},
            'èˆ’é€‚ç‰ˆ': {'OP-FRIDGE': 0, 'WHEEL': 1, 'OP-LuxGift': 1},
            'è±ªåç‰ˆ': {'OP-FRIDGE': 1, 'WHEEL': 0, 'OP-LuxGift': 1},
            'æ——èˆ°ç‰ˆ': {'OP-FRIDGE': 1, 'WHEEL': 1, 'OP-LuxGift': 1}
        }
        
        elasticity_data = []
        
        for scenario_name, config in config_scenarios.items():
            # è®¡ç®—è¯¥é…ç½®çš„é¢„æœŸä»·æ ¼
            price_adjustment = 0
            for feature, value in config.items():
                if feature in self.feature_columns:
                    feature_coef = self.lr_results['feature_importance'][
                        self.lr_results['feature_importance']['feature'] == feature
                    ]['coefficient'].iloc[0]
                    price_adjustment += feature_coef * value
            
            predicted_price = base_price + price_adjustment
            
            # åŸºäºä»·æ ¼é¢„æµ‹é”€é‡ï¼ˆç®€åŒ–çš„å¼¹æ€§æ¨¡å‹ï¼‰
            # å‡è®¾ä»·æ ¼å¼¹æ€§ç³»æ•°ä¸º-1.5ï¼ˆä»·æ ¼æ¯å¢åŠ 1%ï¼Œé”€é‡å‡å°‘1.5%ï¼‰
            price_change_pct = (predicted_price - base_price) / base_price
            volume_change_pct = -1.5 * price_change_pct
            base_volume = len(self.filtered_data)
            predicted_volume = base_volume * (1 + volume_change_pct)
            
            elasticity_data.append({
                'scenario': scenario_name,
                'price': predicted_price / 10000,  # è½¬æ¢ä¸ºä¸‡å…ƒ
                'volume': max(0, predicted_volume),
                'config': config
            })
        
        self.elasticity_df = pd.DataFrame(elasticity_data)
        return self.elasticity_df
    
    def generate_visualizations(self):
        """ç”ŸæˆPlotlyå¯è§†åŒ–å›¾è¡¨"""
        print("\n=== ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===")
        
        # 1. ç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=('çº¿æ€§å›å½’ç‰¹å¾é‡è¦æ€§ï¼ˆç»å¯¹ç³»æ•°å€¼ï¼‰', 'XGBoostç‰¹å¾é‡è¦æ€§'),
            horizontal_spacing=0.1
        )
        
        # çº¿æ€§å›å½’ç‰¹å¾é‡è¦æ€§
        lr_importance = self.lr_results['feature_importance'].copy()
        fig.add_trace(
            go.Bar(
                y=lr_importance['feature'],
                x=lr_importance['abs_coefficient'],
                orientation='h',
                name='çº¿æ€§å›å½’',
                marker_color='lightblue',
                text=[f'{x:.0f}' for x in lr_importance['abs_coefficient']],
                textposition='outside'
            ),
            row=1, col=1
        )
        
        # XGBoostç‰¹å¾é‡è¦æ€§
        xgb_importance = self.xgb_results['feature_importance'].copy()
        fig.add_trace(
            go.Bar(
                y=xgb_importance['feature'],
                x=xgb_importance['importance'],
                orientation='h',
                name='XGBoost',
                marker_color='lightcoral',
                text=[f'{x:.3f}' for x in xgb_importance['importance']],
                textposition='outside'
            ),
            row=1, col=2
        )
        
        fig.update_layout(
            title_text="ç‰¹å¾é‡è¦æ€§å¯¹æ¯”åˆ†æ",
            height=600,
            showlegend=False,
            font=dict(family="Arial, sans-serif", size=12)
        )
        
        fig.update_xaxes(title_text="ç»å¯¹ç³»æ•°å€¼", row=1, col=1)
        fig.update_xaxes(title_text="é‡è¦æ€§åˆ†æ•°", row=1, col=2)
        
        fig.write_html(os.path.join(self.output_dir, 'feature_importance_comparison.html'))
        fig.write_image(os.path.join(self.output_dir, 'feature_importance_comparison.png'))
        
        # 2. é…ç½®æ”¶ç›Šè´¡çŒ®å¯è§†åŒ–
        contrib_df = self.contribution_df
        
        fig = go.Figure()
        
        # åˆ›å»ºæ°”æ³¡å›¾ï¼šxè½´ä¸ºä»·æ ¼è´¡çŒ®ï¼Œyè½´ä¸ºé”€é‡å½±å“ï¼Œæ°”æ³¡å¤§å°ä¸ºXGBoosté‡è¦æ€§
        fig.add_trace(go.Scatter(
            x=contrib_df['price_contribution'],
            y=contrib_df['sales_impact'],
            mode='markers+text',
            marker=dict(
                size=contrib_df['xgb_importance'] * 1000,  # æ”¾å¤§æ°”æ³¡
                color=contrib_df['price_contribution'],
                colorscale='RdYlBu',
                showscale=True,
                colorbar=dict(title="ä»·æ ¼è´¡çŒ®(ä¸‡å…ƒ)")
            ),
            text=contrib_df['feature'],
            textposition="middle center",
            textfont=dict(size=10, color="white"),
            hovertemplate='<b>%{text}</b><br>' +
                         'ä»·æ ¼è´¡çŒ®: %{x:.2f}ä¸‡å…ƒ<br>' +
                         'é”€é‡å½±å“: %{y:.1f}%<br>' +
                         'é‡è¦æ€§: %{marker.size:.3f}<extra></extra>'
        ))
        
        fig.update_layout(
            title="é…ç½®é¡¹æ”¶ç›Šè´¡çŒ®åˆ†æ",
            xaxis_title="å¯¹æˆäº¤ä»·è´¡çŒ®ï¼ˆä¸‡å…ƒï¼‰",
            yaxis_title="å¯¹é”€é‡å½±å“ï¼ˆ%ï¼‰",
            height=600,
            font=dict(family="Arial, sans-serif", size=12)
        )
        
        # æ·»åŠ è±¡é™åˆ†å‰²çº¿
        fig.add_hline(y=5, line_dash="dash", line_color="gray", opacity=0.5)
        fig.add_vline(x=0.3, line_dash="dash", line_color="gray", opacity=0.5)
        
        fig.write_html(os.path.join(self.output_dir, 'configuration_contribution_analysis.html'))
        fig.write_image(os.path.join(self.output_dir, 'configuration_contribution_analysis.png'))
        
        # 3. ä»·æ ¼-é”€é‡å“åº”æ›²çº¿
        elasticity_df = self.elasticity_df
        
        fig = go.Figure()
        
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
        
        for i, (scenario, data) in enumerate(elasticity_df.groupby('scenario')):
            fig.add_trace(go.Scatter(
                x=data['price'],
                y=data['volume'],
                mode='markers+lines',
                name=scenario,
                marker=dict(size=12, color=colors[i % len(colors)]),
                line=dict(width=3, color=colors[i % len(colors)])
            ))
        
        fig.update_layout(
            title="ä»·æ ¼-é”€é‡å“åº”æ›²çº¿ï¼ˆPrice Elasticity Curveï¼‰",
            xaxis_title="å”®ä»·ï¼ˆä¸‡å…ƒï¼‰",
            yaxis_title="é¢„æµ‹é”€é‡",
            height=600,
            font=dict(family="Arial, sans-serif", size=12),
            hovermode='x unified'
        )
        
        fig.write_html(os.path.join(self.output_dir, 'price_elasticity_curve.html'))
        fig.write_image(os.path.join(self.output_dir, 'price_elasticity_curve.png'))
        
        # 4. é”€é‡åˆ†æçƒ­åŠ›å›¾
        sales_summary = []
        for feature in self.feature_columns:
            sales_data = self.sales_analysis[feature]
            for option, data in sales_data.iterrows():
                sales_summary.append({
                    'feature': feature,
                    'option': str(option),
                    'orders': data['unique_orders'],
                    'market_share': data['market_share'],
                    'avg_price': data['avg_price']
                })
        
        sales_df = pd.DataFrame(sales_summary)
        
        # åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®
        pivot_data = sales_df.pivot_table(
            index='feature', 
            columns='option', 
            values='market_share', 
            fill_value=0
        )
        
        fig = go.Figure(data=go.Heatmap(
            z=pivot_data.values,
            x=pivot_data.columns,
            y=pivot_data.index,
            colorscale='Blues',
            text=[[f'{val:.1f}%' for val in row] for row in pivot_data.values],
            texttemplate="%{text}",
            textfont={"size": 10},
            colorbar=dict(title="å¸‚åœºä»½é¢(%)")
        ))
        
        fig.update_layout(
            title="é…ç½®é€‰é¡¹å¸‚åœºä»½é¢çƒ­åŠ›å›¾",
            xaxis_title="é…ç½®é€‰é¡¹",
            yaxis_title="é…ç½®é¡¹",
            height=600,
            font=dict(family="Arial, sans-serif", size=12)
        )
        
        fig.write_html(os.path.join(self.output_dir, 'sales_heatmap.html'))
        fig.write_image(os.path.join(self.output_dir, 'sales_heatmap.png'))
        
        print(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {self.output_dir}")
        print("ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶:")
        print("- feature_importance_comparison.html/png")
        print("- configuration_contribution_analysis.html/png") 
        print("- price_elasticity_curve.html/png")
        print("- sales_heatmap.html/png")
    
    def generate_report(self):
        """ç”Ÿæˆä¸“ä¸šåˆ†ææŠ¥å‘Š"""
        print("\n=== ç”Ÿæˆåˆ†ææŠ¥å‘Š ===")
        
        report_path = os.path.join(self.output_dir, 'cm2_configuration_impact_analysis_report.md')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# ğŸš— CM2é…ç½®é¡¹æ”¶ç›Šè´¡çŒ®åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")
            
            # æ•°æ®æ¦‚è§ˆ
            f.write("## ğŸ“Š ä¸€ã€æ•°æ®æ¦‚è§ˆ\n\n")
            f.write(f"- **åŸå§‹æ•°æ®é‡**: {len(self.raw_data):,} æ¡è®°å½•\n")
            f.write(f"- **è¿‡æ»¤åæ•°æ®é‡**: {len(self.filtered_data):,} æ¡è®°å½•\n")
            f.write(f"- **æ•°æ®ä¿ç•™ç‡**: {len(self.filtered_data)/len(self.raw_data)*100:.2f}%\n")
            f.write(f"- **å”¯ä¸€è®¢å•æ•°**: {self.filtered_data['order_number'].nunique():,} ä¸ª\n")
            f.write(f"- **ä»·æ ¼èŒƒå›´**: {self.filtered_data[self.target_column].min()/10000:.1f} - {self.filtered_data[self.target_column].max()/10000:.1f} ä¸‡å…ƒ\n")
            f.write(f"- **å¹³å‡ä»·æ ¼**: {self.filtered_data[self.target_column].mean()/10000:.1f} ä¸‡å…ƒ\n\n")
            
            # æ¨¡å‹æ€§èƒ½å¯¹æ¯”
            f.write("## ğŸ¯ äºŒã€æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n\n")
            f.write("| æ¨¡å‹ | è®­ç»ƒé›†RÂ² | æµ‹è¯•é›†RÂ² | RMSE | MAE | äº¤å‰éªŒè¯RÂ² |\n")
            f.write("|------|----------|----------|------|-----|------------|\n")
            
            lr_metrics = self.lr_results['metrics']
            f.write(f"| çº¿æ€§å›å½’ | {lr_metrics['train_r2']:.4f} | {lr_metrics['test_r2']:.4f} | "
                   f"{lr_metrics['test_rmse']/10000:.2f}ä¸‡ | {lr_metrics['test_mae']/10000:.2f}ä¸‡ | {lr_metrics['cv_r2_mean']:.4f} |\n")
            
            xgb_metrics = self.xgb_results['metrics']
            f.write(f"| XGBoost | {xgb_metrics['train_r2']:.4f} | {xgb_metrics['test_r2']:.4f} | "
                   f"{xgb_metrics['test_rmse']/10000:.2f}ä¸‡ | {xgb_metrics['test_mae']/10000:.2f}ä¸‡ | {xgb_metrics['cv_r2_mean']:.4f} |\n\n")
            
            f.write("> ğŸ’¡ **æ¨¡å‹é€‰æ‹©å»ºè®®**: ")
            if xgb_metrics['test_r2'] > lr_metrics['test_r2']:
                f.write(f"XGBoostæ¨¡å‹è¡¨ç°æ›´ä¼˜ï¼ˆRÂ² {xgb_metrics['test_r2']:.4f}ï¼‰ï¼Œå»ºè®®ç”¨äºä»·æ ¼é¢„æµ‹\n\n")
            else:
                f.write(f"çº¿æ€§å›å½’æ¨¡å‹è¡¨ç°æ›´ä¼˜ï¼ˆRÂ² {lr_metrics['test_r2']:.4f}ï¼‰ï¼Œå»ºè®®ç”¨äºä»·æ ¼é¢„æµ‹\n\n")
            
            f.write("---\n\n")
            
            # æ ¸å¿ƒåˆ†æç»“æœ
            f.write("## ğŸ§­ ä¸‰ã€æ ¸å¿ƒåˆ†æç»“æœ\n\n")
            
            # é…ç½®æ”¶ç›Šè´¡çŒ®è¡¨
            f.write("### 1ï¸âƒ£ é…ç½®æ”¶ç›Šè´¡çŒ®è¡¨\n\n")
            f.write("| é…ç½®é¡¹ | å¯¹æˆäº¤ä»·è´¡çŒ®ï¼ˆä¸‡å…ƒï¼‰ | å¯¹é”€é‡å½±å“ï¼ˆ%ï¼‰ | ç»“è®º |\n")
            f.write("|--------|---------------------|----------------|------|\n")
            
            for _, row in self.contribution_df.iterrows():
                f.write(f"| {row['feature']} | {row['price_contribution']:+.1f} | {row['sales_impact']:+.1f}% | {row['conclusion']} |\n")
            
            f.write("\n> ğŸ’¡ **è¿™å¼ è¡¨æœ€èƒ½æ‰“åŠ¨å†³ç­–è€…**ï¼šå“ªäº›é…ç½®\"å€¼å¾—åŠ é’±\"ï¼Œå“ªäº›\"æ¶ˆè´¹è€…ä¸åœ¨æ„\"ã€‚\n\n")
            
            f.write("---\n\n")
            
            # ä»·æ ¼-é”€é‡å“åº”æ›²çº¿
            f.write("### 2ï¸âƒ£ ä»·æ ¼-é”€é‡å“åº”æ›²çº¿ï¼ˆPrice Elasticity Curveï¼‰\n\n")
            f.write("é€šè¿‡æ¨¡æ‹Ÿä¸åŒé…ç½®ç»„åˆçš„ä»·æ ¼ â†’ é”€é‡æ›²çº¿ï¼Œ\n")
            f.write("å¯ä»¥å¾—åˆ°æ¯ä¸ªé…ç½®åŒ…çš„\"æ€§ä»·æ¯”æœ€ä¼˜ç‚¹\"ã€‚\n\n")
            
            f.write("| é…ç½®æ–¹æ¡ˆ | é¢„æµ‹å”®ä»·ï¼ˆä¸‡å…ƒï¼‰ | é¢„æµ‹é”€é‡ | æ€§ä»·æ¯”è¯„åˆ† |\n")
            f.write("|----------|------------------|----------|------------|\n")
            
            for _, row in self.elasticity_df.iterrows():
                # è®¡ç®—æ€§ä»·æ¯”è¯„åˆ†ï¼ˆé”€é‡/ä»·æ ¼çš„å½’ä¸€åŒ–å€¼ï¼‰
                performance_score = (row['volume'] / row['price']) / 1000
                f.write(f"| {row['scenario']} | {row['price']:.1f} | {row['volume']:.0f} | {performance_score:.2f} |\n")
            
            f.write("\n```\nXè½´ï¼šå”®ä»·ï¼ˆä¸‡å…ƒï¼‰\nYè½´ï¼šé”€é‡é¢„æµ‹\nä¸åŒé¢œè‰²ï¼šé…ç½®ç»„åˆæ–¹æ¡ˆ\n```\n\n")
            f.write("ğŸ‘‰ **å¸®åŠ©å›ç­”**ï¼š\"æ——èˆ°ç‰ˆå†åŠ æ™ºèƒ½é©¾é©¶åŒ…ï¼Œé”€é‡ä¼šä¸ä¼šæ‰å¤ªå¤šï¼Ÿ\"\n\n")
            
            f.write("---\n\n")
            
            # è¯¦ç»†é”€é‡åˆ†æ
            f.write("## ğŸ“ˆ å››ã€è¯¦ç»†é”€é‡åˆ†æ\n\n")
            
            for feature in self.feature_columns:
                f.write(f"### {feature}\n\n")
                sales_data = self.sales_analysis[feature]
                
                f.write("| é…ç½®é€‰é¡¹ | è®¢å•æ•°é‡ | å¸‚åœºä»½é¢ | å¹³å‡ä»·æ ¼ | ä¸­ä½ä»·æ ¼ |\n")
                f.write("|----------|----------|----------|----------|----------|\n")
                
                for option, data in sales_data.iterrows():
                    f.write(f"| {option} | {data['unique_orders']:,} | {data['market_share']:.1f}% | "
                           f"{data['avg_price']/10000:.1f}ä¸‡ | {data['median_price']/10000:.1f}ä¸‡ |\n")
                f.write("\n")
            
            f.write("---\n\n")
            
            # å•†ä¸šæ´å¯Ÿå’Œå»ºè®®
            f.write("## ğŸ’¡ äº”ã€å•†ä¸šæ´å¯Ÿå’Œå»ºè®®\n\n")
            
            f.write("### ğŸ¯ å…³é”®å‘ç°\n\n")
            
            # æ‰¾å‡ºæœ€é‡è¦çš„é…ç½®é¡¹
            top_feature = self.contribution_df.iloc[0]
            f.write(f"1. **æœ€é‡è¦é…ç½®é¡¹**: {top_feature['feature']} \n")
            f.write(f"   - ä»·æ ¼è´¡çŒ®: {top_feature['price_contribution']:+.1f}ä¸‡å…ƒ\n")
            f.write(f"   - é”€é‡å½±å“: {top_feature['sales_impact']:+.1f}%\n")
            f.write(f"   - å»ºè®®: {top_feature['conclusion']}\n\n")
            
            # æ‰¾å‡ºæœ€å—æ¬¢è¿çš„é…ç½®
            f.write("2. **çƒ­é—¨é…ç½®ç»„åˆ**:\n")
            for feature in self.feature_columns[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                sales_data = self.sales_analysis[feature]
                top_option = sales_data.loc[sales_data['market_share'].idxmax()]
                f.write(f"   - {feature}: {top_option.name} (å¸‚åœºä»½é¢ {top_option['market_share']:.1f}%)\n")
            f.write("\n")
            
            # ä»·æ ¼å¼¹æ€§æ´å¯Ÿ
            best_scenario = self.elasticity_df.loc[self.elasticity_df['volume'].idxmax()]
            f.write(f"3. **æœ€ä¼˜æ€§ä»·æ¯”æ–¹æ¡ˆ**: {best_scenario['scenario']}\n")
            f.write(f"   - é¢„æµ‹ä»·æ ¼: {best_scenario['price']:.1f}ä¸‡å…ƒ\n")
            f.write(f"   - é¢„æµ‹é”€é‡: {best_scenario['volume']:.0f}å°\n\n")
            
            f.write("### ğŸš€ è¡ŒåŠ¨å»ºè®®\n\n")
            f.write("#### çŸ­æœŸç­–ç•¥ï¼ˆ1-3ä¸ªæœˆï¼‰\n")
            f.write("1. **å®šä»·ä¼˜åŒ–**: é‡ç‚¹è°ƒæ•´é«˜è´¡çŒ®é…ç½®é¡¹çš„ä»·æ ¼æ¢¯åº¦\n")
            f.write("2. **åº“å­˜è°ƒæ•´**: æ ¹æ®å¸‚åœºä»½é¢æ•°æ®ä¼˜åŒ–å„é…ç½®é€‰é¡¹åº“å­˜æ¯”ä¾‹\n")
            f.write("3. **è¥é”€é‡ç‚¹**: çªå‡ºæ¨å¹¿é«˜ä»·å€¼ã€é«˜æ¥å—åº¦çš„é…ç½®ç»„åˆ\n\n")
            
            f.write("#### ä¸­æœŸç­–ç•¥ï¼ˆ3-6ä¸ªæœˆï¼‰\n")
            f.write("1. **äº§å“ç»„åˆä¼˜åŒ–**: è€ƒè™‘å°†çƒ­é—¨é…ç½®æ ‡å‡†åŒ–åˆ°ä¸­é«˜é…è½¦å‹\n")
            f.write("2. **ä»·æ ¼ç­–ç•¥è°ƒæ•´**: åŸºäºå¼¹æ€§åˆ†æç»“æœè°ƒæ•´ä¸åŒé…ç½®æ–¹æ¡ˆå®šä»·\n")
            f.write("3. **å®¢æˆ·ç»†åˆ†**: é’ˆå¯¹ä¸åŒä»·æ ¼æ•æ„Ÿåº¦å®¢æˆ·æ¨èåˆé€‚é…ç½®\n\n")
            
            f.write("#### é•¿æœŸç­–ç•¥ï¼ˆ6ä¸ªæœˆä»¥ä¸Šï¼‰\n")
            f.write("1. **äº§å“è§„åˆ’**: åŸºäºé…ç½®è´¡çŒ®åˆ†ææŒ‡å¯¼ä¸‹ä¸€ä»£äº§å“å¼€å‘\n")
            f.write("2. **ä¾›åº”é“¾ä¼˜åŒ–**: è°ƒæ•´é«˜ä»·å€¼é…ç½®çš„ä¾›åº”å•†åˆä½œç­–ç•¥\n")
            f.write("3. **å“ç‰Œå®šä½**: å¼ºåŒ–é«˜è´¡çŒ®é…ç½®é¡¹çš„å“ç‰Œä»·å€¼ä¼ æ’­\n\n")
            
            f.write("---\n\n")
            f.write("**æŠ¥å‘Šè¯´æ˜**: æœ¬åˆ†æåŸºäºå†å²é”€å”®æ•°æ®ï¼Œå»ºè®®ç»“åˆå¸‚åœºè°ƒç ”å’Œç«å“åˆ†æè¿›è¡Œå†³ç­–ã€‚\n")
        
        print(f"ä¸“ä¸šåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        # ä¿å­˜æ•°æ®ç»“æœ
        results_data = {
            'linear_regression_importance': self.lr_results['feature_importance'],
            'xgboost_importance': self.xgb_results['feature_importance'],
            'sales_analysis': self.sales_analysis
        }
        
        # ä¿å­˜ä¸ºExcelæ–‡ä»¶
        excel_path = os.path.join(self.output_dir, 'cm2_configuration_analysis_results.xlsx')
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            results_data['linear_regression_importance'].to_excel(writer, sheet_name='çº¿æ€§å›å½’é‡è¦æ€§', index=False)
            results_data['xgboost_importance'].to_excel(writer, sheet_name='XGBoosté‡è¦æ€§', index=False)
            
            for feature, data in results_data['sales_analysis'].items():
                data.to_excel(writer, sheet_name=f'{feature}_é”€é‡åˆ†æ')
        
        print(f"æ•°æ®ç»“æœå·²ä¿å­˜åˆ°: {excel_path}")
        
        return report_path, excel_path
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        print("å¼€å§‹CM2é…ç½®é¡¹å½±å“åˆ†æ...")
        
        try:
            # 1. æ•°æ®åŠ è½½å’Œç­›é€‰
            self.load_and_filter_data()
            
            # 2. ç‰¹å¾å‡†å¤‡
            self.prepare_features()
            
            # 3. çº¿æ€§å›å½’åˆ†æ
            self.linear_regression_analysis()
            
            # 4. XGBoostå›å½’åˆ†æ
            self.xgboost_regression_analysis()
            
            # 5. é”€é‡åˆ†æ
            self.sales_volume_analysis()
            
            # 6. è®¡ç®—é…ç½®æ”¶ç›Šè´¡çŒ®
            self.calculate_configuration_contribution()
            
            # 7. ç”Ÿæˆä»·æ ¼å¼¹æ€§åˆ†æ
            self.generate_price_elasticity_analysis()
            
            # 8. ç”Ÿæˆå¯è§†åŒ–
            self.generate_visualizations()
            
            # 9. ç”ŸæˆæŠ¥å‘Š
            report_path, excel_path = self.generate_report()
            
            print(f"\n=== åˆ†æå®Œæˆ ===")
            print(f"åˆ†ææŠ¥å‘Š: {report_path}")
            print(f"æ•°æ®ç»“æœ: {excel_path}")
            print(f"å›¾è¡¨æ–‡ä»¶: {self.output_dir}")
            
            return True
            
        except Exception as e:
            print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='CM2é…ç½®é¡¹å½±å“åˆ†æ')
    parser.add_argument('-i', '--input', required=True, help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ - {args.input}")
        sys.exit(1)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = CM2ConfigurationAnalyzer(args.input, args.output)
    
    # è¿è¡Œåˆ†æ
    success = analyzer.run_full_analysis()
    
    if success:
        print("åˆ†ææˆåŠŸå®Œæˆï¼")
        sys.exit(0)
    else:
        print("åˆ†æå¤±è´¥ï¼")
        sys.exit(1)


if __name__ == '__main__':
    main()