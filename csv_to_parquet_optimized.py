import pandas as pd
import os
from collections import defaultdict

def analyze_columns_globally(csv_file_path, sample_size=1000000):
    """
    å…¨å±€åˆ†æåˆ—çš„ç‰¹å¾ï¼Œå†³å®šæœ€ä½³æ•°æ®ç±»å‹
    """
    print(f"ğŸ” åˆ†ææ•°æ®é›†ç‰¹å¾: {csv_file_path}")
    
    # è¯»å–æ ·æœ¬æ•°æ®è¿›è¡Œåˆ†æ
    chunk_size = 100000
    column_stats = defaultdict(lambda: {'unique_values': set(), 'total_count': 0, 'numeric_count': 0})
    
    sample_count = 0
    for chunk in pd.read_csv(csv_file_path, chunksize=chunk_size):
        for col in chunk.columns:
            if chunk[col].dtype == 'object':
                # æ”¶é›†å”¯ä¸€å€¼ï¼ˆé™åˆ¶æ•°é‡é¿å…å†…å­˜é—®é¢˜ï¼‰
                unique_vals = chunk[col].dropna().unique()
                if len(column_stats[col]['unique_values']) < 10000:
                    column_stats[col]['unique_values'].update(unique_vals[:1000])
                
                column_stats[col]['total_count'] += len(chunk[col])
                
                # æ£€æŸ¥æ•°å€¼è½¬æ¢å¯èƒ½æ€§
                numeric_converted = pd.to_numeric(chunk[col], errors='coerce')
                column_stats[col]['numeric_count'] += numeric_converted.notna().sum()
        
        sample_count += len(chunk)
        if sample_count >= sample_size:
            break
    
    # å†³å®šæ¯åˆ—çš„æœ€ä½³ç±»å‹
    column_types = {}
    for col, stats in column_stats.items():
        unique_count = len(stats['unique_values'])
        total_count = stats['total_count']
        numeric_ratio = stats['numeric_count'] / total_count if total_count > 0 else 0
        
        if numeric_ratio > 0.8:
            column_types[col] = 'numeric'
        elif unique_count <= 1000 or (unique_count / total_count) < 0.5:
            column_types[col] = 'category'
        else:
            column_types[col] = 'object'
        
        print(f"  {col}: {unique_count} å”¯ä¸€å€¼, æ•°å€¼æ¯”ä¾‹ {numeric_ratio:.2%} -> {column_types[col]}")
    
    return column_types

def csv_to_parquet_with_global_optimization(csv_file_path, parquet_output_path):
    """
    åŸºäºå…¨å±€åˆ†æä¼˜åŒ–CSVåˆ°Parquetè½¬æ¢
    """
    print(f"ğŸ”„ æ­£åœ¨è¯»å–CSVæ–‡ä»¶: {csv_file_path}")
    
    # ç¬¬ä¸€æ­¥ï¼šå…¨å±€åˆ†æ
    column_types = analyze_columns_globally(csv_file_path)
    
    # ç¬¬äºŒæ­¥ï¼šåˆ†å—å¤„ç†å¹¶åº”ç”¨ä¼˜åŒ–
    chunk_size = 100000
    chunks = []
    
    try:
        for chunk in pd.read_csv(csv_file_path, chunksize=chunk_size):
            # ä¼˜åŒ–æ—¥æœŸåˆ—
            if 'æ—¥æœŸ' in chunk.columns:
                chunk['æ—¥æœŸ'] = pd.to_datetime(chunk['æ—¥æœŸ'], errors='coerce')
            
            # æ ¹æ®å…¨å±€åˆ†æç»“æœä¼˜åŒ–åˆ—ç±»å‹
            for col in chunk.columns:
                if col in column_types:
                    if column_types[col] == 'numeric':
                        chunk[col] = pd.to_numeric(chunk[col], errors='coerce')
                    elif column_types[col] == 'category':
                        chunk[col] = chunk[col].astype('category')
                    # objectç±»å‹ä¿æŒä¸å˜
            
            chunks.append(chunk)
            print(f"ğŸ“ˆ å·²å¤„ç† {len(chunks) * chunk_size} è¡Œ...")
    
        # åˆå¹¶æ‰€æœ‰å—
        print("ğŸ”„ æ­£åœ¨åˆå¹¶æ•°æ®...")
        df = pd.concat(chunks, ignore_index=True)
        
        # é‡æ–°åº”ç”¨categoryç±»å‹ï¼ˆconcatå¯èƒ½ä¼šé‡ç½®ç±»å‹ï¼‰
        print("ğŸ”„ é‡æ–°åº”ç”¨æ•°æ®ç±»å‹ä¼˜åŒ–...")
        for col, target_type in column_types.items():
            if col in df.columns:
                if target_type == 'category':
                    df[col] = df[col].astype('category')
                elif target_type == 'numeric':
                    df[col] = pd.to_numeric(df[col], errors='coerce')
        
        print(f"ğŸ“Š æ•°æ®ç»´åº¦: {df.shape[0]} è¡Œ x {df.shape[1]} åˆ—")
        
        # æ˜¾ç¤ºæœ€ç»ˆæ•°æ®ç±»å‹
        print("\nğŸ“‹ ä¼˜åŒ–åæ•°æ®ç±»å‹:")
        category_count = 0
        object_count = 0
        for col, dtype in df.dtypes.items():
            print(f"  {col}: {dtype}")
            if str(dtype) == 'category':
                category_count += 1
            elif str(dtype) == 'object':
                object_count += 1
        
        print(f"\nğŸ“Š ç±»å‹ç»Ÿè®¡: {category_count} ä¸ªcategoryåˆ—, {object_count} ä¸ªobjectåˆ—")
        
        # ä¿å­˜ä¸ºParquetæ–‡ä»¶
        output_dir = os.path.dirname(parquet_output_path)
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜Parquetæ–‡ä»¶: {parquet_output_path}")
        df.to_parquet(parquet_output_path, index=False, engine='pyarrow', compression='snappy')
        
        # éªŒè¯ä¿å­˜ç»“æœ
        file_size = os.path.getsize(parquet_output_path) / (1024 * 1024)  # MB
        print(f"âœ… è½¬æ¢å®Œæˆï¼æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_usage = df.memory_usage(deep=True).sum() / (1024 * 1024)
        print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {memory_usage:.2f} MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {str(e)}")
        return False

if __name__ == "__main__":
    csv_file = "/Users/zihao_/Documents/coding/dataset/formatted/æœˆåº¦_ä¸Šé™©é‡_0723.csv"
    parquet_file = "/Users/zihao_/Documents/coding/dataset/formatted/æœˆåº¦_ä¸Šé™©é‡_0723_optimized.parquet"
    
    # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(csv_file):
        print(f"âŒ é”™è¯¯: CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        exit(1)
    
    print("ğŸš€ å¼€å§‹å…¨å±€ä¼˜åŒ–CSVåˆ°Parquetè½¬æ¢...")
    print("=" * 50)
    
    success = csv_to_parquet_with_global_optimization(csv_file, parquet_file)
    
    if success:
        print("\nğŸ‰ è½¬æ¢æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {parquet_file}")
    else:
        print("\nâŒ è½¬æ¢å¤±è´¥")
        exit(1)